import pandas as pd
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import os
import sys
from typing import List, Optional, Tuple, Dict, Any
from pathlib import Path

from app.titanic.titanic_dataset import TitanicDataset
from app.titanic.titanic_method import TitanicMethod

# ê³µí†µ ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

# ë¡œê¹… ì„¤ì •
try:
    from common.utils import setup_logging
    logger = setup_logging("titanic_service")
except ImportError:
    import logging
    logger = logging.getLogger("titanic_service")
    logger.setLevel(logging.INFO)
    if not logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)


class TitanicService:
    """íƒ€ì´íƒ€ë‹‰ ìŠ¹ê° ë°ì´í„° ì²˜ë¦¬ ë° ML ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.processed_data = None  # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
        self.train_label = None  # ì›ë³¸ trainì˜ Survived ë ˆì´ë¸”
        self.models = {}  # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
    
    # ==================== ML íŒŒì´í”„ë¼ì¸ ====================
    
    # ==================== PREPROCESS ====================
    def _collect_dataframe_info(self, df: pd.DataFrame, sample_size: int = 5) -> Dict[str, Any]:
        """
        DataFrame ì •ë³´ë¥¼ JSON-serializable ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        
        Args:
            df: pandas DataFrame
            sample_size: ìƒ˜í”Œ ë°ì´í„° í–‰ ê°œìˆ˜
            
        Returns:
            DataFrame ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # ìƒ˜í”Œ ë°ì´í„° ë³€í™˜ (NaNì„ Noneìœ¼ë¡œ ë³€í™˜)
            sample_data = df.head(sample_size).replace({np.nan: None}).to_dict('records')
            
            # ì»¬ëŸ¼ ì •ë³´ (íƒ€ì… í¬í•¨)
            columns_info = {
                col: str(df[col].dtype) for col in df.columns
            }
            
            return {
                "columns": df.columns.tolist(),
                "column_count": len(df.columns),
                "row_count": len(df),
                "shape": list(df.shape),
                "null_count": int(df.isnull().sum().sum()),
                "columns_info": columns_info,
                "sample_data": sample_data
            }
        except Exception as e:
            logger.warning(f"DataFrame ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {str(e)}")
            return {
                "columns": df.columns.tolist() if hasattr(df, 'columns') else [],
                "column_count": len(df.columns) if hasattr(df, 'columns') else 0,
                "row_count": len(df) if hasattr(df, '__len__') else 0,
                "shape": list(df.shape) if hasattr(df, 'shape') else [0, 0],
                "null_count": 0,
                "columns_info": {},
                "sample_data": []
            }
    
    def _calculate_changes(self, before_info: Dict[str, Any], after_info: Dict[str, Any], 
                          removed_columns: List[str], preprocessing_steps: List[str]) -> Dict[str, Any]:
        """
        ì „ì²˜ë¦¬ ì „í›„ ë³€í™” ì •ë³´ ê³„ì‚°
        
        Args:
            before_info: ì „ì²˜ë¦¬ ì „ ì •ë³´
            after_info: ì „ì²˜ë¦¬ í›„ ì •ë³´
            removed_columns: ì œê±°ëœ ì»¬ëŸ¼ ëª©ë¡
            preprocessing_steps: ìˆ˜í–‰ëœ ì „ì²˜ë¦¬ ë‹¨ê³„ ëª©ë¡
            
        Returns:
            ë³€í™” ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        before_cols = set(before_info.get("columns", []))
        after_cols = set(after_info.get("columns", []))
        
        added_columns = list(after_cols - before_cols)
        actually_removed = list(before_cols - after_cols)
        
        return {
            "columns_removed": len(actually_removed),
            "columns_added": len(added_columns),
            "removed_column_names": actually_removed,
            "added_column_names": added_columns,
            "nulls_filled": before_info.get("null_count", 0) - after_info.get("null_count", 0),
            "preprocessing_steps": preprocessing_steps
        }
    
    def preprocess(self):
        """
        íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰
        Returns:
            ì „ì²˜ë¦¬ ê²°ê³¼ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        logger.info("â–¶ ì „ì²˜ë¦¬ ì‹œì‘")
        
        the_method = TitanicMethod()
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        data_path = Path(__file__).parent.parent / 'resources' / 'titanic'
        train_path = data_path / 'train.csv'
        test_path = data_path / 'test.csv'
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not train_path.exists():
            logger.error(f"train.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_path}")
            return {"status": "error", "message": f"train.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_path}"}
        if not test_path.exists():
            logger.error(f"test.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_path}")
            return {"status": "error", "message": f"test.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_path}"}
        
        # ë°ì´í„° ì½ê¸°
        df_train = the_method.read_csv(str(train_path))
        df_test = the_method.read_csv(str(test_path))
        
        # ì›ë³¸ trainì—ì„œ Survived ë¼ë²¨ ì €ì¥
        self.train_label = df_train['Survived'].copy()
        
        this_train = the_method.create_df(df_train, 'Survived')
        this_test = the_method.create_df(df_test, 'Survived') if 'Survived' in df_test.columns else df_test
        

        # ì „ì²˜ë¦¬ ì „ ë°ì´í„° ì •ë³´ ìˆ˜ì§‘
        before_info = self._collect_dataframe_info(this_train, sample_size=5)
        before_info_test = self._collect_dataframe_info(this_test, sample_size=5)
        null_count = int(this_train.isnull().sum().sum())
        null_count_test = int(this_test.isnull().sum().sum())
        
        logger.info(f"ğŸ“Š ì „ì²˜ë¦¬ ì „ | Train: {this_train.shape[0]}í–‰Ã—{this_train.shape[1]}ì—´ (ê²°ì¸¡ì¹˜: {null_count:,}) | Test: {this_test.shape[0]}í–‰Ã—{this_test.shape[1]}ì—´ (ê²°ì¸¡ì¹˜: {null_count_test:,})")
        
        # DataFrame ê°„ê²°í•˜ê²Œ ì¶œë ¥ (3í–‰ë§Œ, í•µì‹¬ ì»¬ëŸ¼ë§Œ)
        sample_df = this_train.head(3).replace({np.nan: None})
        if len(sample_df.columns) > 8:
            # ì»¬ëŸ¼ì´ ë§ìœ¼ë©´ ì¼ë¶€ë§Œ í‘œì‹œ
            key_cols = ['PassengerId', 'Pclass', 'Age', 'Fare', 'Sex', 'Embarked'] + [c for c in sample_df.columns if c not in ['PassengerId', 'Pclass', 'Age', 'Fare', 'Sex', 'Embarked']][:2]
            key_cols = [c for c in key_cols if c in sample_df.columns]
            sample_df = sample_df[key_cols]
        df_str = sample_df.to_string(index=False, justify='left', max_colwidth=15)
        logger.info(f"Train ìƒ˜í”Œ (ìƒìœ„ 3í–‰):\n{df_str}")


        # ì „ì²˜ë¦¬ ì „ ë°ì´í„° ê°ì²´ ìƒì„±
        this = TitanicDataset()
        this.train = this_train
        this.test = this_test
        
        # ê²°ì¸¡ì¹˜ í™•ì¸
        the_method.check_null(this)

        # ì „ì²˜ë¦¬ ìˆ˜í–‰
        logger.info("â–¶ ì „ì²˜ë¦¬ ìˆ˜í–‰ ì¤‘...")
        preprocessing_steps = []
        
        drop_features = ['SibSp', 'Parch', 'Ticket', 'Cabin']
        this = the_method.drop_features(this, *drop_features)
        preprocessing_steps.append(f"drop_features: {drop_features}")
        
        this = the_method.pclass_ordinal(this)
        preprocessing_steps.append("pclass_ordinal")
        
        this = the_method.fare_ordinal(this)
        preprocessing_steps.append("fare_ordinal")
        
        this = the_method.embarked_ordinal(this)
        preprocessing_steps.append("embarked_ordinal")
        
        this = the_method.gender_nominal(this)
        preprocessing_steps.append("gender_nominal")
        
        this = the_method.age_ratio(this)
        preprocessing_steps.append("age_ratio")
        
        this = the_method.title_nominal(this)
        preprocessing_steps.append("title_nominal")

        drop_name = ['Name']
        this = the_method.drop_features(this, *drop_name)
        preprocessing_steps.append(f"drop_features: {drop_name}")




        # ì „ì²˜ë¦¬ í›„ ì •ë³´ ìˆ˜ì§‘
        logger.info("â–¶ ì „ì²˜ë¦¬ ì™„ë£Œ!")
        
        # ê²°ì¸¡ì¹˜ í™•ì¸
        the_method.check_null(this)
        
        # train.csv ì •ë³´
        after_info = self._collect_dataframe_info(this.train, sample_size=5)
        null_count_after = int(this.train.isnull().sum().sum())
        
        # test.csv ì •ë³´
        after_info_test = self._collect_dataframe_info(this.test, sample_size=5)
        null_count_after_test = int(this.test.isnull().sum().sum())
        
        logger.info(f"ğŸ“Š ì „ì²˜ë¦¬ í›„ | Train: {this.train.shape[0]}í–‰Ã—{this.train.shape[1]}ì—´ (ê²°ì¸¡ì¹˜: {null_count_after:,}) | Test: {this.test.shape[0]}í–‰Ã—{this.test.shape[1]}ì—´ (ê²°ì¸¡ì¹˜: {null_count_after_test:,})")
        
        # DataFrame ê°„ê²°í•˜ê²Œ ì¶œë ¥ (3í–‰ë§Œ)
        sample_df_after = this.train.head(3).replace({np.nan: None})
        if len(sample_df_after.columns) > 8:
            key_cols = ['PassengerId', 'Pclass', 'Fare', 'Embarked_encoded', 'Gender_encoded', 'Age_encoded', 'Title_encoded']
            key_cols = [c for c in key_cols if c in sample_df_after.columns][:8]
            sample_df_after = sample_df_after[key_cols]
        df_str_after = sample_df_after.to_string(index=False, justify='left', max_colwidth=15)
        logger.info(f"Train ìƒ˜í”Œ (ìƒìœ„ 3í–‰):\n{df_str_after}")


        # ë³€í™” ì •ë³´ ê³„ì‚°
        removed_columns = drop_features + drop_name
        changes_info = self._calculate_changes(
            before_info, 
            after_info, 
            removed_columns, 
            preprocessing_steps
        )
        
        # ë³€í™” ìš”ì•½ ì¶œë ¥
        logger.info(f"ğŸ“ˆ ë³€í™” ìš”ì•½ | ì œê±°: {changes_info['columns_removed']}ê°œ | ì¶”ê°€: {changes_info['columns_added']}ê°œ | ê²°ì¸¡ì¹˜ ì²˜ë¦¬: {changes_info['nulls_filled']:,}ê°œ")
        logger.info(f"ë‹¨ê³„: {' â†’ '.join([s.split(':')[0] if ':' in s else s for s in preprocessing_steps])}")
        
        # ìµœì¢… ì‘ë‹µ êµ¬ì„±
        response_data = {
            "status": "success",
            "message": "ì „ì²˜ë¦¬ ì™„ë£Œ",
            "data": {
                "before_preprocessing": before_info,
                "after_preprocessing": after_info,
                "changes": changes_info
            }
        }
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
        self.processed_data = this
        
        logger.info("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ")
        return response_data


    def modeling(self):
        """ëª¨ë¸ ìƒì„±"""
        logger.info("â–¶ ëª¨ë¸ ìƒì„± ì‹œì‘")
        
        from sklearn.svm import SVC
        
        # ëª¨ë¸ ìƒì„± (SVMë§Œ ì‚¬ìš©)
        self.models = {
            'SVM': SVC(random_state=42, probability=True)
        }
        
        logger.info("âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ: SVM")

    def learning(self):
        """ëª¨ë¸ í•™ìŠµ"""
        if self.processed_data is None:
            logger.error("ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € preprocess()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return None
        
        if self.train_label is None:
            logger.error("í•™ìŠµ ë ˆì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € preprocess()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return None
        
        logger.info("â–¶ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        
        X_train = self.processed_data.train
        y_train = self.train_label
        
        # PassengerId ì œê±° (í•™ìŠµì— ë¶ˆí•„ìš”)
        if 'PassengerId' in X_train.columns:
            X_train = X_train.drop(columns=['PassengerId'])
        
        # ëª¨ë¸ í•™ìŠµ
        for name, model in self.models.items():
            logger.debug(f"í•™ìŠµ ì¤‘: {name}")
            model.fit(X_train, y_train)
        
        logger.info(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {len(self.models)}ê°œ ëª¨ë¸")

    def evaluate(self):
        """ëª¨ë¸ í‰ê°€"""
        if self.processed_data is None:
            logger.error("ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € preprocess()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return None
        
        if self.train_label is None:
            logger.error("í•™ìŠµ ë ˆì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € preprocess()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return None
        
        if not self.models:
            logger.error("í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € modeling()ê³¼ learning()ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
            return None
        
        logger.info("â–¶ ëª¨ë¸ í‰ê°€ ì‹œì‘")
        
        from sklearn.model_selection import train_test_split
        
        X = self.processed_data.train
        y = self.train_label
        
        # PassengerId ì œê±°
        if 'PassengerId' in X.columns:
            X = X.drop(columns=['PassengerId'])
        
        # Train/Validation ë¶„ë¦¬
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        results = {}
        best_model_name = None
        best_model = None
        best_accuracy = 0
        
        # ê° ëª¨ë¸ í‰ê°€
        for name, model in self.models.items():
            # ì¬í•™ìŠµ (train ë°ì´í„°ë¡œ)
            model.fit(X_train, y_train)
            
            # ê²€ì¦ ë°ì´í„°ë¡œ í‰ê°€
            accuracy = model.score(X_val, y_val)
            results[name] = accuracy
            
            logger.info(f"{name} í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {accuracy:.4f} ({accuracy*100:.2f}%)")
            
            # ìµœê³  ëª¨ë¸ ì—…ë°ì´íŠ¸
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_model_name = name
                best_model = model
        
        logger.info("âœ… ëª¨ë¸ í‰ê°€ ì™„ë£Œ")
        
        # ìµœê³  ëª¨ë¸ë¡œ test ë°ì´í„° ì˜ˆì¸¡ ë° CSV ì €ì¥
        submission_path = self.generate_submission_csv(best_model, best_model_name)
        
        return {
            "status": "success",
            "results": results,
            "best_model": best_model_name,
            "best_accuracy": best_accuracy,
            "submission_file": submission_path
        }


    def generate_submission_csv(self, model, model_name: str) -> str:
        """ìµœê³  ëª¨ë¸ë¡œ test ë°ì´í„° ì˜ˆì¸¡í•˜ì—¬ ìºê¸€ ì œì¶œìš© CSV ìƒì„±"""
        if self.processed_data is None:
            logger.error("ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        logger.info(f"â–¶ {model_name} ëª¨ë¸ë¡œ test ë°ì´í„° ì˜ˆì¸¡ ì‹œì‘")
        
        # ì „ì²´ train ë°ì´í„°ë¡œ ìµœì¢… í•™ìŠµ
        X_train = self.processed_data.train
        y_train = self.train_label
        
        # PassengerId ì œê±°
        if 'PassengerId' in X_train.columns:
            X_train = X_train.drop(columns=['PassengerId'])
        
        # ìµœì¢… í•™ìŠµ
        model.fit(X_train, y_train)
        
        # Test ë°ì´í„° ì¤€ë¹„
        X_test = self.processed_data.test.copy()
        
        # Testì˜ PassengerId ì €ì¥
        test_passenger_ids = X_test['PassengerId'].copy() if 'PassengerId' in X_test.columns else None
        
        # PassengerId ì œê±°
        if 'PassengerId' in X_test.columns:
            X_test = X_test.drop(columns=['PassengerId'])
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        predictions = model.predict(X_test)
        
        # CSV ì €ì¥
        download_dir = Path(__file__).parent / 'download'
        download_dir.mkdir(exist_ok=True)
        
        # íŒŒì¼ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"submission_{model_name}_{timestamp}.csv"
        filepath = download_dir / filename
        
        # DataFrame ìƒì„±
        submission_df = pd.DataFrame({
            'PassengerId': test_passenger_ids,
            'Survived': predictions
        })
        
        # CSV ì €ì¥
        submission_df.to_csv(filepath, index=False)
        
        logger.info(f"âœ… ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ: {filepath}")
        logger.info(f"   ì˜ˆì¸¡ ê°œìˆ˜: {len(predictions)}ê°œ")
        logger.info(f"   ìƒì¡´ ì˜ˆì¸¡: {predictions.sum()}ëª…, ì‚¬ë§ ì˜ˆì¸¡: {len(predictions) - predictions.sum()}ëª…")
        
        return str(filepath)

    def submit(self):
        """ê²°ê³¼ ì œì¶œ"""
        logger.info("ì œì¶œ ì‹œì‘")
        logger.info("ì œì¶œ ì™„ë£Œ")
