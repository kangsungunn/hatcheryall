# NLTK ê¸°ë°˜ ì›Œë“œí´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ êµ¬ì¶• í”„ë¡œì íŠ¸

**ì‘ì„±ì¼:** 2025ë…„ 12ì›” 11ì¼  
**í”„ë¡œì íŠ¸:** Emma ë§ë­‰ì¹˜ ìì—°ì–´ ì²˜ë¦¬ ë° ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™”  
**ê¸°ìˆ  ìŠ¤íƒ:** Python, FastAPI, NLTK, WordCloud, PIL, Docker

---

## ğŸ“‹ Table of Contents

1. [í”„ë¡œì íŠ¸ ê°œìš”](#í”„ë¡œì íŠ¸-ê°œìš”)
2. [ì‚¬ì „ ì¤€ë¹„](#ì‚¬ì „-ì¤€ë¹„)
3. [ë‹¨ê³„ë³„ êµ¬í˜„ ê³¼ì •](#ë‹¨ê³„ë³„-êµ¬í˜„-ê³¼ì •)
   - [Step 1: ì ˆì°¨ì  ì½”ë“œë¥¼ OOPë¡œ ë¦¬íŒ©í† ë§](#step-1-ì ˆì°¨ì -ì½”ë“œë¥¼-oopë¡œ-ë¦¬íŒ©í† ë§)
   - [Step 2: NLPService í´ë˜ìŠ¤ êµ¬í˜„](#step-2-nlpservice-í´ë˜ìŠ¤-êµ¬í˜„)
   - [Step 3: ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ë° ì €ì¥ ê¸°ëŠ¥](#step-3-ì›Œë“œí´ë¼ìš°ë“œ-ìƒì„±-ë°-ì €ì¥-ê¸°ëŠ¥)
   - [Step 4: FastAPI ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„](#step-4-fastapi-ì—”ë“œí¬ì¸íŠ¸-êµ¬í˜„)
4. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)
5. [ìµœì¢… ê²°ê³¼ë¬¼](#ìµœì¢…-ê²°ê³¼ë¬¼)
6. [í•™ìŠµ ì •ë¦¬](#í•™ìŠµ-ì •ë¦¬)
7. [ë‹¤ìŒ í•™ìŠµ ê³¼ì œ](#ë‹¤ìŒ-í•™ìŠµ-ê³¼ì œ)

---

## í”„ë¡œì íŠ¸ ê°œìš”

### ë¬´ì—‡ì„ ë§Œë“¤ì—ˆë‚˜?

ì œì¸ ì˜¤ìŠ¤í‹´ì˜ ì†Œì„¤ "Emma" ë§ë­‰ì¹˜ë¥¼ ë¶„ì„í•˜ì—¬ ë“±ì¥ì¸ë¬¼ ì´ë¦„ì˜ ë¹ˆë„ë¥¼ ì›Œë“œí´ë¼ìš°ë“œë¡œ ì‹œê°í™”í•˜ëŠ” ì›¹ ì„œë¹„ìŠ¤ë¥¼ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. NLTK(Natural Language Toolkit)ë¥¼ í™œìš©í•œ ì „ë¬¸ì ì¸ ìì—°ì–´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ OOP ë°©ì‹ìœ¼ë¡œ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤.

### ì™œ ë§Œë“¤ì—ˆë‚˜?

**ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜:**
- í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ì£¼ìš” í‚¤ì›Œë“œë¥¼ ì‹œê°ì ìœ¼ë¡œ íŒŒì•…
- ëŒ€ìš©ëŸ‰ ë¬¸ì„œì˜ í•µì‹¬ ê°œë…ì„ ë¹ ë¥´ê²Œ ì´í•´
- ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ ì§ê´€ì ìœ¼ë¡œ ì „ë‹¬
- ì›¹ APIë¥¼ í†µí•œ ì„œë¹„ìŠ¤í™”ë¡œ í™•ì¥ì„± í™•ë³´

**ê¸°ìˆ ì  ëª©í‘œ:**
- ì ˆì°¨ì  ì½”ë“œë¥¼ ê°ì²´ì§€í–¥(OOP) êµ¬ì¡°ë¡œ ì „í™˜
- NLTKì˜ ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ ê¸°ëŠ¥ í•™ìŠµ
- ì¬ì‚¬ìš© ê°€ëŠ¥í•œ NLP ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ ì„¤ê³„
- FastAPIì™€ NLTKì˜ í†µí•©
- Docker í™˜ê²½ì—ì„œ NLTK ë°ì´í„° ê´€ë¦¬

**í•™ìŠµ ëª©ì :**
- NLTK íŒ¨í‚¤ì§€ì˜ í•µì‹¬ ê¸°ëŠ¥ ìŠµë“
- í˜•íƒœì†Œ ë¶„ì„(Stemming, Lemmatization) ì´í•´
- í’ˆì‚¬ íƒœê¹…(POS Tagging) í™œìš©
- ë¹ˆë„ ë¶„ì„(FreqDist) ë° ì‹œê°í™”

### ì£¼ìš” ê¸°ëŠ¥

1. **ë§ë­‰ì¹˜(Corpus) ê´€ë¦¬**
   - Gutenberg ë§ë­‰ì¹˜ ë¡œë“œ
   - ë‹¤ì–‘í•œ ë¬¸í•™ ì‘í’ˆ ì ‘ê·¼
   - í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬

2. **í† í°í™”(Tokenization)**
   - ë¬¸ì¥ ë‹¨ìœ„ í† í°í™”
   - ë‹¨ì–´ ë‹¨ìœ„ í† í°í™”
   - ì •ê·œì‹ ê¸°ë°˜ í† í°í™”

3. **í˜•íƒœì†Œ ë¶„ì„**
   - Porter Stemmer: ì–´ê°„ ì¶”ì¶œ
   - Lancaster Stemmer: ê°•ë ¥í•œ ì–´ê°„ ì¶”ì¶œ
   - Lemmatization: ì›í˜• ë³µì›

4. **í’ˆì‚¬ íƒœê¹…(POS Tagging)**
   - Penn Treebank íƒœê·¸ì…‹ ì‚¬ìš©
   - ê³ ìœ ëª…ì‚¬(NNP) í•„í„°ë§
   - Stopwords ì œê±°

5. **ë¹ˆë„ ë¶„ì„**
   - FreqDist í´ë˜ìŠ¤ í™œìš©
   - ì¶œí˜„ ë¹ˆë„ ê³„ì‚°
   - Top N ë‹¨ì–´ ì¶”ì¶œ

6. **ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™”**
   - ë¹ˆë„ ê¸°ë°˜ í¬ê¸° ì¡°ì •
   - ì»¤ìŠ¤í„°ë§ˆì´ì§• ì˜µì…˜ (í¬ê¸°, ìƒ‰ìƒ, ëœë¤ ì‹œë“œ)
   - PNG ì´ë¯¸ì§€ ìƒì„± ë° ì €ì¥

### ê¸°ìˆ  ìŠ¤íƒ

| ì¹´í…Œê³ ë¦¬ | ê¸°ìˆ  | ì‚¬ìš© ëª©ì  |
|---------|------|----------|
| **ë°±ì—”ë“œ** | FastAPI | RESTful API ì„œë²„ |
| **ìì—°ì–´ ì²˜ë¦¬** | NLTK | í† í°í™”, í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹… |
| **ì‹œê°í™”** | WordCloud | ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€ ìƒì„± |
| **ì´ë¯¸ì§€ ì²˜ë¦¬** | PIL (Pillow) | ì´ë¯¸ì§€ í¬ë§· ë³€í™˜ |
| **ì»¨í…Œì´ë„ˆí™”** | Docker | ì„œë¹„ìŠ¤ ë°°í¬ ë° NLTK ë°ì´í„° ê´€ë¦¬ |
| **API Gateway** | Spring Cloud Gateway | ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ë¼ìš°íŒ… |

---

## ì‚¬ì „ ì¤€ë¹„

### 1. í•„ìš”í•œ íŒŒì¼ êµ¬ì¡°

```
kroaddy_project_dacon/
â”œâ”€â”€ ai.kroaddy.site/
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ mlservice/
â”‚           â”œâ”€â”€ app/
â”‚           â”‚   â””â”€â”€ nlp/
â”‚           â”‚       â”œâ”€â”€ emma/
â”‚           â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚           â”‚       â”‚   â””â”€â”€ emma_wordcloud.py    # NLPService í´ë˜ìŠ¤
â”‚           â”‚       â”œâ”€â”€ samsung/                  # í–¥í›„ í™•ì¥ìš©
â”‚           â”‚       â”œâ”€â”€ data/
â”‚           â”‚       â”‚   â”œâ”€â”€ D2Coding.ttf         # í•œê¸€ í°íŠ¸
â”‚           â”‚       â”‚   â”œâ”€â”€ kr-Report_2018.txt   # í•œê¸€ ë§ë­‰ì¹˜
â”‚           â”‚       â”‚   â””â”€â”€ stopwords.txt        # ë¶ˆìš©ì–´ ëª©ë¡
â”‚           â”‚       â”œâ”€â”€ save/                     # ì›Œë“œí´ë¼ìš°ë“œ ì €ì¥
â”‚           â”‚       â”‚   â””â”€â”€ emma_wordcloud_*.png
â”‚           â”‚       â”œâ”€â”€ __init__.py
â”‚           â”‚       â””â”€â”€ nlp_router.py            # API ì—”ë“œí¬ì¸íŠ¸
â”‚           â””â”€â”€ requirements.txt
â””â”€â”€ docker-compose.yaml
```

### 2. NLTK íŒ¨í‚¤ì§€ë€?

**NLTK (Natural Language Toolkit):**
- êµìœ¡ìš©ìœ¼ë¡œ ê°œë°œëœ ìì—°ì–´ ì²˜ë¦¬ íŒ¨í‚¤ì§€
- ë‹¤ì–‘í•œ ë§ë­‰ì¹˜ì™€ ë¶„ì„ ë„êµ¬ ì œê³µ
- ì—°êµ¬ ë° ì‹¤ë¬´ì—ì„œ ë„ë¦¬ ì‚¬ìš©

**ì£¼ìš” ê¸°ëŠ¥:**
1. **ë§ë­‰ì¹˜(Corpus)**: ìƒ˜í”Œ ë¬¸ì„œ ì§‘í•©
2. **í† í° ìƒì„±(Tokenization)**: ë¬¸ìì—´ì„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
3. **í˜•íƒœì†Œ ë¶„ì„(Morphological Analysis)**: ì–´ê·¼, ì ‘ë‘ì‚¬, ì ‘ë¯¸ì‚¬ ë¶„ì„
4. **í’ˆì‚¬ íƒœê¹…(POS Tagging)**: ë‹¨ì–´ì˜ í’ˆì‚¬ ìë™ ë¶€ì°©

### 3. í•„ìš”í•œ Python íŒ¨í‚¤ì§€

```txt
# requirements.txt
nltk>=3.8.0          # ìì—°ì–´ ì²˜ë¦¬
wordcloud>=1.9.0     # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
sentencepiece>=0.1.99 # ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì €
konlpy>=0.6.0        # í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬
opencv-python>=4.8.0 # ì´ë¯¸ì§€ ì²˜ë¦¬
Pillow>=10.0.0       # ì´ë¯¸ì§€ í¬ë§· ë³€í™˜
jpype1>=1.4.0        # Javaì™€ Python ì—°ë™ (KoNLPyìš©)
```

### 4. NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ

NLTKëŠ” ì‚¬ìš© ì „ì— í•„ìš”í•œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤:

```python
import nltk

# í•„ìˆ˜ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
nltk.download('book')                           # ìƒ˜í”Œ ë§ë­‰ì¹˜ ëª¨ìŒ
nltk.download('punkt')                          # ë¬¸ì¥/ë‹¨ì–´ í† í¬ë‚˜ì´ì €
nltk.download('wordnet')                        # WordNet ì‚¬ì „
nltk.download('averaged_perceptron_tagger')     # í’ˆì‚¬ íƒœê±°
nltk.download('omw-1.4')                        # Open Multilingual Wordnet
```

**ë‹¤ìš´ë¡œë“œ ìœ„ì¹˜:**
- Linux/Mac: `~/nltk_data/`
- Windows: `C:\Users\{username}\nltk_data\`
- Docker ì»¨í…Œì´ë„ˆ: `/root/nltk_data/`

---

## ë‹¨ê³„ë³„ êµ¬í˜„ ê³¼ì •

### Step 1: ì ˆì°¨ì  ì½”ë“œë¥¼ OOPë¡œ ë¦¬íŒ©í† ë§

#### ğŸ¯ ëª©í‘œ
ê¸°ì¡´ ì ˆì°¨ì (Procedural) ë°©ì‹ì˜ NLTK ì½”ë“œë¥¼ ê°ì²´ì§€í–¥(OOP) ë°©ì‹ì˜ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í´ë˜ìŠ¤ë¡œ ì „í™˜

#### ğŸ“ ì™œ OOPë¡œ ë¦¬íŒ©í† ë§í•˜ë‚˜?

**ì ˆì°¨ì  ì½”ë“œì˜ ë¬¸ì œì :**
- âŒ ì½”ë“œ ì¤‘ë³µ: ê°™ì€ ë¡œì§ì„ ì—¬ëŸ¬ ê³³ì—ì„œ ë°˜ë³µ
- âŒ ìœ ì§€ë³´ìˆ˜ ì–´ë ¤ì›€: ìˆ˜ì • ì‹œ ì—¬ëŸ¬ ê³³ì„ ì°¾ì•„ë‹¤ë…€ì•¼ í•¨
- âŒ ì¬ì‚¬ìš© ë¶ˆê°€: ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•˜ê¸° ì–´ë ¤ì›€
- âŒ í…ŒìŠ¤íŠ¸ ì–´ë ¤ì›€: í•¨ìˆ˜ê°€ ë…ë¦½ì ì´ì§€ ì•ŠìŒ

**OOPì˜ ì¥ì :**
- âœ… ìº¡ìŠí™”: ê´€ë ¨ ë°ì´í„°ì™€ ë©”ì„œë“œë¥¼ í•˜ë‚˜ë¡œ ë¬¶ìŒ
- âœ… ì¬ì‚¬ìš©ì„±: í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì—¬ëŸ¬ ê³³ì—ì„œ ì‚¬ìš©
- âœ… ìœ ì§€ë³´ìˆ˜: í•œ ê³³ë§Œ ìˆ˜ì •í•˜ë©´ ì „ì²´ ë°˜ì˜
- âœ… í™•ì¥ì„±: ìƒì†ì„ í†µí•œ ê¸°ëŠ¥ í™•ì¥ ê°€ëŠ¥
- âœ… í…ŒìŠ¤íŠ¸: ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„± ìš©ì´

#### ğŸ’» ë¦¬íŒ©í† ë§ ì „í›„ ë¹„êµ

**Before (ì ˆì°¨ì  ì½”ë“œ):**

```python
# nlp_service.py (ì ˆì°¨ì  ë°©ì‹)
import nltk
from wordcloud import WordCloud

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
nltk.download('book')
nltk.download('punkt')

# ë§ë­‰ì¹˜ ë¡œë“œ
emma_raw = nltk.corpus.gutenberg.raw('austen-emma.txt')

# í† í°í™”
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer("[\w]+")
emma_tokens = tokenizer.tokenize(emma_raw)

# í’ˆì‚¬ íƒœê¹…
from nltk import pos_tag
tagged_tokens = pos_tag(emma_tokens)

# ê³ ìœ ëª…ì‚¬ í•„í„°ë§
stopwords = ["Mr.", "Mrs.", "Miss"]
names = [word for word, tag in tagged_tokens 
         if tag == "NNP" and word not in stopwords]

# ë¹ˆë„ ë¶„ì„
from nltk import FreqDist
fd_names = FreqDist(names)

# ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
wc = WordCloud(width=1000, height=600)
wc.generate_from_frequencies(fd_names)
wc.to_file('wordcloud.png')
```

**ë¬¸ì œì :**
- ì „ì—­ ë³€ìˆ˜ ì‚¬ìš©ìœ¼ë¡œ ìƒíƒœ ê´€ë¦¬ ì–´ë ¤ì›€
- ê°™ì€ ì‘ì—…ì„ ë°˜ë³µí•  ë•Œë§ˆë‹¤ ì „ì²´ ì½”ë“œ ì¬ì‹¤í–‰
- ë‹¤ë¥¸ ë§ë­‰ì¹˜ ë¶„ì„ ì‹œ ì½”ë“œ ë³µì‚¬/ìˆ˜ì • í•„ìš”
- ì—ëŸ¬ ì²˜ë¦¬ ì—†ìŒ

**After (OOP ë°©ì‹):**

```python
# emma_wordcloud.py (ê°ì²´ì§€í–¥ ë°©ì‹)
class NLPService:
    """
    NLTK ê¸°ë°˜ ìì—°ì–´ ì²˜ë¦¬ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
    """
    
    def __init__(self, download_nltk_data: bool = True):
        """ì´ˆê¸°í™” ë° NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ"""
        if download_nltk_data:
            self._download_nltk_data()
        
        # ë¶„ì„ ë„êµ¬ ì´ˆê¸°í™”
        self.porter_stemmer = PorterStemmer()
        self.lancaster_stemmer = LancasterStemmer()
        self.lemmatizer = WordNetLemmatizer()
        self.regex_tokenizer = RegexpTokenizer("[\w]+")
        
        # ë‚´ë¶€ ìƒíƒœ ì €ì¥
        self.current_text: Optional[Text] = None
        self.current_tokens: Optional[List[str]] = None
        self.current_corpus: Optional[str] = None
        
        # save í´ë” ì„¤ì •
        self.save_dir = Path(__file__).parent.parent / 'save'
        self.save_dir.mkdir(exist_ok=True)
    
    def load_corpus(self, fileid: str) -> str:
        """ë§ë­‰ì¹˜ ë¡œë“œ"""
        raw_text = nltk.corpus.gutenberg.raw(fileid)
        self.current_corpus = raw_text
        return raw_text
    
    def tokenize_regex(self, text: Optional[str] = None) -> List[str]:
        """ì •ê·œì‹ ê¸°ë°˜ í† í°í™”"""
        if text is None:
            text = self.current_corpus
        tokens = self.regex_tokenizer.tokenize(text)
        self.current_tokens = tokens
        return tokens
    
    def pos_tag(self, tokens: Optional[List[str]] = None) -> List[Tuple[str, str]]:
        """í’ˆì‚¬ íƒœê¹…"""
        if tokens is None:
            tokens = self.current_tokens
        return pos_tag(tokens)
    
    def filter_tokens_by_pos(
        self, 
        pos_tag: str, 
        stopwords: Optional[List[str]] = None,
        tagged_list: Optional[List[Tuple[str, str]]] = None
    ) -> List[str]:
        """íŠ¹ì • í’ˆì‚¬ë§Œ í•„í„°ë§"""
        if tagged_list is None:
            tagged_list = self.pos_tag()
        if stopwords is None:
            stopwords = []
        return [word for word, tag in tagged_list 
                if tag == pos_tag and word not in stopwords]
    
    def create_freq_dist(self, tokens: List[str]) -> FreqDist:
        """ë¹ˆë„ ë¶„í¬ ìƒì„±"""
        return FreqDist(tokens)
    
    def generate_wordcloud(
        self, 
        freq_dist: FreqDist,
        width: int = 1000,
        height: int = 600,
        show: bool = True
    ) -> WordCloud:
        """ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
        wc = WordCloud(width=width, height=height)
        wc.generate_from_frequencies(freq_dist)
        
        # ìë™ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"emma_wordcloud_{timestamp}.png"
        filepath = self.save_dir / filename
        wc.to_file(str(filepath))
        
        return wc


# ì‚¬ìš© ì˜ˆì‹œ
nlp = NLPService()
emma_raw = nlp.load_corpus('austen-emma.txt')
tokens = nlp.tokenize_regex(emma_raw)
tagged = nlp.pos_tag(tokens)
names = nlp.filter_tokens_by_pos('NNP', stopwords=["Mr.", "Mrs."], tagged_list=tagged)
fd = nlp.create_freq_dist(names)
wc = nlp.generate_wordcloud(fd)
```

**ê°œì„  ì‚¬í•­:**
- âœ… ìƒíƒœ ê´€ë¦¬: `self.current_corpus`, `self.current_tokens`ë¡œ ìƒíƒœ ì €ì¥
- âœ… ì¬ì‚¬ìš©ì„±: `nlp = NLPService()`ë¡œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í›„ ë°˜ë³µ ì‚¬ìš©
- âœ… ìœ ì—°ì„±: íŒŒë¼ë¯¸í„°ë¡œ ë™ì‘ ì»¤ìŠ¤í„°ë§ˆì´ì§•
- âœ… ìë™í™”: ì›Œë“œí´ë¼ìš°ë“œ ìë™ ì €ì¥
- âœ… ì—ëŸ¬ ì²˜ë¦¬: ê° ë©”ì„œë“œì—ì„œ ì…ë ¥ ê²€ì¦

---

### Step 2: NLPService í´ë˜ìŠ¤ êµ¬í˜„

#### ğŸ¯ ëª©í‘œ
NLTKì˜ ì£¼ìš” ê¸°ëŠ¥ì„ ë©”ì„œë“œë¡œ êµ¬í˜„í•œ ì™„ì „í•œ NLP ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ ì‘ì„±

#### ğŸ’» í´ë˜ìŠ¤ êµ¬ì¡° ì„¤ê³„

```python
class NLPService:
    """
    NLTK ê¸°ë°˜ ìì—°ì–´ ì²˜ë¦¬ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
    
    êµ¬ì¡°:
    â”œâ”€â”€ __init__: ì´ˆê¸°í™” ë° ë„êµ¬ ì„¤ì •
    â”œâ”€â”€ ë§ë­‰ì¹˜ ê´€ë¦¬
    â”‚   â”œâ”€â”€ get_corpus_fileids()
    â”‚   â””â”€â”€ load_corpus()
    â”œâ”€â”€ í† í°í™”
    â”‚   â”œâ”€â”€ tokenize_sentences()
    â”‚   â”œâ”€â”€ tokenize_words()
    â”‚   â””â”€â”€ tokenize_regex()
    â”œâ”€â”€ í˜•íƒœì†Œ ë¶„ì„
    â”‚   â”œâ”€â”€ stem_porter()
    â”‚   â”œâ”€â”€ stem_lancaster()
    â”‚   â””â”€â”€ lemmatize()
    â”œâ”€â”€ í’ˆì‚¬ íƒœê¹…
    â”‚   â”œâ”€â”€ pos_tag()
    â”‚   â”œâ”€â”€ extract_nouns()
    â”‚   â””â”€â”€ filter_tokens_by_pos()
    â”œâ”€â”€ í…ìŠ¤íŠ¸ ë¶„ì„
    â”‚   â”œâ”€â”€ create_text_analyzer()
    â”‚   â”œâ”€â”€ plot_word_frequency()
    â”‚   â””â”€â”€ plot_dispersion()
    â”œâ”€â”€ ë¹ˆë„ ë¶„ì„
    â”‚   â”œâ”€â”€ create_freq_dist()
    â”‚   â”œâ”€â”€ get_freq_statistics()
    â”‚   â””â”€â”€ get_most_common()
    â””â”€â”€ ì›Œë“œí´ë¼ìš°ë“œ
        â”œâ”€â”€ generate_wordcloud()
        â””â”€â”€ save_wordcloud()
    """
```

#### ğŸ“ ì£¼ìš” ë©”ì„œë“œ ìƒì„¸ ì„¤ëª…

**1. ì´ˆê¸°í™” ë©”ì„œë“œ**

```python
def __init__(self, download_nltk_data: bool = True):
    """
    NLPService ì´ˆê¸°í™”
    
    Args:
        download_nltk_data: NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì—¬ë¶€
    """
    # 1. NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    if download_nltk_data:
        try:
            nltk.download('book', quiet=True)
            nltk.download('punkt', quiet=True)
            nltk.download('wordnet', quiet=True)
            nltk.download('averaged_perceptron_tagger', quiet=True)
            nltk.download('omw-1.4', quiet=True)
        except Exception as e:
            print(f"NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
    
    # 2. í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
    self.porter_stemmer = PorterStemmer()
    self.lancaster_stemmer = LancasterStemmer()
    self.lemmatizer = WordNetLemmatizer()
    
    # 3. í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
    self.regex_tokenizer = RegexpTokenizer("[\w]+")
    
    # 4. ë‚´ë¶€ ìƒíƒœ ì €ì¥ ë³€ìˆ˜
    self.current_text: Optional[Text] = None
    self.current_tokens: Optional[List[str]] = None
    self.current_corpus: Optional[str] = None
    
    # 5. save í´ë” ê²½ë¡œ ì„¤ì •
    self.save_dir = Path(__file__).parent.parent / 'save'
    self.save_dir.mkdir(exist_ok=True)
```

**ì½”ë“œ ì„¤ëª…:**

1. **NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ:**
   - `quiet=True`: ë‹¤ìš´ë¡œë“œ ì§„í–‰ ìƒí™© ë©”ì‹œì§€ ìˆ¨ê¹€
   - `try-except`: ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨í•´ë„ í”„ë¡œê·¸ë¨ ê³„ì† ì‹¤í–‰
   - Docker í™˜ê²½ì—ì„œëŠ” ë³¼ë¥¨ ë§ˆìš´íŠ¸ë¡œ ë°ì´í„° ì˜ì†í™”

2. **ë¶„ì„ ë„êµ¬ ì´ˆê¸°í™”:**
   - `PorterStemmer`: ì˜ì–´ ì–´ê°„ ì¶”ì¶œ (ì˜ˆ: "running" â†’ "run")
   - `LancasterStemmer`: ë” ê°•ë ¥í•œ ì–´ê°„ ì¶”ì¶œ
   - `WordNetLemmatizer`: ì›í˜• ë³µì› (ì˜ˆ: "better" â†’ "good")

3. **ë‚´ë¶€ ìƒíƒœ ê´€ë¦¬:**
   - `current_corpus`: í˜„ì¬ ë¡œë“œëœ í…ìŠ¤íŠ¸
   - `current_tokens`: í˜„ì¬ í† í° ë¦¬ìŠ¤íŠ¸
   - `current_text`: NLTK Text ê°ì²´
   - ë©”ì„œë“œ ì²´ì´ë‹ ê°€ëŠ¥: `nlp.load().tokenize().pos_tag()`

4. **save í´ë” ì„¤ì •:**
   - `Path(__file__).parent.parent`: í˜„ì¬ íŒŒì¼ì˜ 2ë‹¨ê³„ ìƒìœ„ í´ë”
   - `emma_wordcloud.py` â†’ `emma/` â†’ `nlp/` â†’ `nlp/save/`
   - `mkdir(exist_ok=True)`: í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±, ìˆìœ¼ë©´ ë¬´ì‹œ

**2. í† í°í™” ë©”ì„œë“œ**

```python
def tokenize_regex(self, text: Optional[str] = None, pattern: str = "[\w]+") -> List[str]:
    """
    ì •ê·œì‹ ê¸°ë°˜ í† í°í™”
    
    Args:
        text: í† í°í™”í•  í…ìŠ¤íŠ¸ (Noneì´ë©´ í˜„ì¬ ë¡œë“œëœ í…ìŠ¤íŠ¸ ì‚¬ìš©)
        pattern: ì •ê·œì‹ íŒ¨í„´ (ê¸°ë³¸ê°’: "[\w]+")
        
    Returns:
        í† í° ë¦¬ìŠ¤íŠ¸
    """
    # 1. í…ìŠ¤íŠ¸ ê²€ì¦
    if text is None:
        if self.current_corpus is None:
            raise ValueError("ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        text = self.current_corpus
    
    # 2. ì •ê·œì‹ í† í¬ë‚˜ì´ì € ìƒì„± ë° í† í°í™”
    tokenizer = RegexpTokenizer(pattern)
    tokens = tokenizer.tokenize(text)
    
    # 3. ìƒíƒœ ì €ì¥
    self.current_tokens = tokens
    
    return tokens
```

**ì •ê·œì‹ íŒ¨í„´ ì„¤ëª…:**

| íŒ¨í„´ | ì˜ë¯¸ | ì˜ˆì‹œ |
|------|------|------|
| `[\w]+` | ë‹¨ì–´ ë¬¸ì (ë¬¸ì, ìˆ«ì, _) 1ê°œ ì´ìƒ | "Hello123" âœ… |
| `[A-Za-z]+` | ì˜ë¬¸ìë§Œ | "Hello" âœ…, "Hello123" âŒ |
| `[\w-]+` | ë‹¨ì–´ ë¬¸ì + í•˜ì´í”ˆ | "mother-in-law" âœ… |

**ì™œ ì •ê·œì‹ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ë‚˜?**
- `word_tokenize()`ëŠ” êµ¬ë‘ì ì„ ë³„ë„ í† í°ìœ¼ë¡œ ë¶„ë¦¬ (ì˜ˆ: "Hello!" â†’ ["Hello", "!"])
- `RegexpTokenizer("[\w]+"))`ëŠ” ë‹¨ì–´ë§Œ ì¶”ì¶œ (ì˜ˆ: "Hello!" â†’ ["Hello"])
- ì›Œë“œí´ë¼ìš°ë“œì—ëŠ” ë‹¨ì–´ë§Œ í•„ìš”í•˜ë¯€ë¡œ ì •ê·œì‹ ë°©ì‹ì´ ì í•©

**3. í’ˆì‚¬ íƒœê¹… ë° í•„í„°ë§**

```python
def filter_tokens_by_pos(
    self, 
    pos_tag: str, 
    stopwords: Optional[List[str]] = None,
    tagged_list: Optional[List[Tuple[str, str]]] = None
) -> List[str]:
    """
    íŠ¹ì • í’ˆì‚¬ë§Œ í•„í„°ë§í•˜ì—¬ ì¶”ì¶œ
    
    Args:
        pos_tag: ì¶”ì¶œí•  í’ˆì‚¬ íƒœê·¸ (ì˜ˆ: 'NNP' - ê³ ìœ ëª…ì‚¬)
        stopwords: ì œì™¸í•  ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
        tagged_list: POS íƒœê¹…ëœ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ìë™ íƒœê¹…)
        
    Returns:
        í•„í„°ë§ëœ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
    """
    # 1. í’ˆì‚¬ íƒœê¹… (í•„ìš”ì‹œ)
    if tagged_list is None:
        tagged_list = self.pos_tag()
    
    # 2. Stopwords ê¸°ë³¸ê°’
    if stopwords is None:
        stopwords = []
    
    # 3. í•„í„°ë§ (ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜)
    return [word for word, tag in tagged_list 
            if tag == pos_tag and word not in stopwords]
```

**Penn Treebank í’ˆì‚¬ íƒœê·¸ ì˜ˆì‹œ:**

| íƒœê·¸ | ì˜ë¯¸ | ì˜ˆì‹œ |
|------|------|------|
| NNP | ë‹¨ìˆ˜ ê³ ìœ ëª…ì‚¬ | Emma, London |
| NNPS | ë³µìˆ˜ ê³ ìœ ëª…ì‚¬ | Americans |
| NN | ì¼ë°˜ ëª…ì‚¬ | book, dog |
| VB | ë™ì‚¬ ì›í˜• | run, eat |
| VBD | ë™ì‚¬ ê³¼ê±°í˜• | ran, ate |
| JJ | í˜•ìš©ì‚¬ | beautiful, good |

**Emma ë§ë­‰ì¹˜ì—ì„œ ê³ ìœ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ëŠ” ì´ìœ :**
- ë“±ì¥ì¸ë¬¼ ì´ë¦„ì— ì§‘ì¤‘
- ì¼ë°˜ ëª…ì‚¬ëŠ” "the", "a", "to" ë“±ì´ ë§ì•„ ì˜ë¯¸ ì—†ìŒ
- "Emma", "Mr. Knightley", "Frank" ë“± ì¸ë¬¼ ê´€ê³„ íŒŒì•…

**4. ë¹ˆë„ ë¶„ì„**

```python
def create_freq_dist(self, tokens: List[str]) -> FreqDist:
    """
    í† í° ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ë¹ˆë„ ë¶„í¬ ìƒì„±
    
    Args:
        tokens: í† í° ë¦¬ìŠ¤íŠ¸
        
    Returns:
        FreqDist ê°ì²´
    """
    return FreqDist(tokens)


def get_freq_statistics(self, freq_dist: FreqDist, word: str) -> Tuple[int, int, float]:
    """
    ë¹ˆë„ í†µê³„ ì •ë³´ ë°˜í™˜
    
    Args:
        freq_dist: FreqDist ê°ì²´
        word: ì¡°íšŒí•  ë‹¨ì–´
        
    Returns:
        (ì „ì²´ ë‹¨ì–´ ìˆ˜, ë‹¨ì–´ ì¶œí˜„ íšŸìˆ˜, ë‹¨ì–´ ì¶œí˜„ í™•ë¥ ) íŠœí”Œ
    """
    total = freq_dist.N()           # ì „ì²´ ë‹¨ì–´ ìˆ˜
    count = freq_dist[word]         # íŠ¹ì • ë‹¨ì–´ ì¶œí˜„ íšŸìˆ˜
    frequency = freq_dist.freq(word)  # ì¶œí˜„ í™•ë¥  (count / total)
    
    return total, count, frequency


def get_most_common(self, freq_dist: FreqDist, num: int = 10) -> List[Tuple[str, int]]:
    """
    ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ ë°˜í™˜
    
    Args:
        freq_dist: FreqDist ê°ì²´
        num: ë°˜í™˜í•  ë‹¨ì–´ ìˆ˜
        
    Returns:
        (ë‹¨ì–´, ë¹ˆë„) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
    """
    return freq_dist.most_common(num)
```

**FreqDist í´ë˜ìŠ¤ë€?**
- NLTKì˜ ë¹ˆë„ ë¶„í¬ í´ë˜ìŠ¤
- Pythonì˜ `Counter`ì™€ ìœ ì‚¬í•˜ì§€ë§Œ ë” ë§ì€ ê¸°ëŠ¥ ì œê³µ
- ë‹¨ì–´ë¥¼ í‚¤(key), ì¶œí˜„ë¹ˆë„ë¥¼ ê°’(value)ìœ¼ë¡œ ì €ì¥

**ì‚¬ìš© ì˜ˆì‹œ:**
```python
# Emma ë§ë­‰ì¹˜ì—ì„œ "Emma" ì¶œí˜„ íšŸìˆ˜
fd = nlp.create_freq_dist(names)
total, emma_count, emma_freq = nlp.get_freq_statistics(fd, "Emma")

print(f"ì „ì²´ ë‹¨ì–´ ìˆ˜: {total}")        # 865
print(f"Emma ì¶œí˜„: {emma_count}")      # 191
print(f"ì¶œí˜„ í™•ë¥ : {emma_freq:.4f}")   # 0.2208 (22.08%)
```

---

### Step 3: ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ë° ì €ì¥ ê¸°ëŠ¥

#### ğŸ¯ ëª©í‘œ
ë¹ˆë„ ë¶„ì„ ê²°ê³¼ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³  ìë™ìœ¼ë¡œ ì €ì¥

#### ğŸ’» ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ë©”ì„œë“œ

```python
def generate_wordcloud(
    self, 
    freq_dist: FreqDist,
    width: int = 1000,
    height: int = 600,
    background_color: str = "white",
    random_state: int = 0,
    show: bool = True
) -> WordCloud:
    """
    ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    
    Args:
        freq_dist: FreqDist ê°ì²´ (ë‹¨ì–´ ë¹ˆë„ ë¶„í¬)
        width: ì´ë¯¸ì§€ ë„ˆë¹„ (í”½ì…€)
        height: ì´ë¯¸ì§€ ë†’ì´ (í”½ì…€)
        background_color: ë°°ê²½ìƒ‰ (ê¸°ë³¸ê°’: "white")
        random_state: ëœë¤ ì‹œë“œ (ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´)
        show: ê·¸ë˜í”„ í‘œì‹œ ì—¬ë¶€
        
    Returns:
        WordCloud ê°ì²´
    """
    # ========================================
    # 1ë‹¨ê³„: WordCloud ê°ì²´ ìƒì„±
    # ========================================
    wc = WordCloud(
        width=width,
        height=height,
        background_color=background_color,
        random_state=random_state
    )
    
    # ========================================
    # 2ë‹¨ê³„: ë¹ˆë„ ë°ì´í„°ë¡œë¶€í„° ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    # ========================================
    wc.generate_from_frequencies(freq_dist)
    
    # ========================================
    # 3ë‹¨ê³„: ìë™ ì €ì¥ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
    # ========================================
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"emma_wordcloud_{timestamp}.png"
    filepath = self.save_dir / filename
    wc.to_file(str(filepath))
    
    # ========================================
    # 4ë‹¨ê³„: ì‹œê°í™” (ì„ íƒì )
    # ========================================
    if show:
        plt.imshow(wc)
        plt.axis("off")  # ì¶• ìˆ¨ê¸°ê¸°
        plt.show()
    
    return wc
```

#### ğŸ“ íŒŒë¼ë¯¸í„° ìƒì„¸ ì„¤ëª…

**1. `width`, `height`:**
```python
# ë‹¤ì–‘í•œ í¬ê¸° ì˜ˆì‹œ
wc = nlp.generate_wordcloud(fd, width=1920, height=1080)  # Full HD
wc = nlp.generate_wordcloud(fd, width=800, height=600)    # 4:3 ë¹„ìœ¨
wc = nlp.generate_wordcloud(fd, width=1000, height=500)   # ì™€ì´ë“œ
```

**2. `background_color`:**
```python
# ë°°ê²½ìƒ‰ ì˜µì…˜
wc = nlp.generate_wordcloud(fd, background_color="white")   # í•˜ì–€ìƒ‰
wc = nlp.generate_wordcloud(fd, background_color="black")   # ê²€ì€ìƒ‰
wc = nlp.generate_wordcloud(fd, background_color="#f0f0f0") # íšŒìƒ‰ (HEX)
```

**3. `random_state`:**
```python
# ëœë¤ ì‹œë“œ - ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼
wc1 = nlp.generate_wordcloud(fd, random_state=0)
wc2 = nlp.generate_wordcloud(fd, random_state=0)
# wc1ê³¼ wc2ëŠ” ì™„ì „íˆ ë™ì¼í•œ ë°°ì¹˜

wc3 = nlp.generate_wordcloud(fd, random_state=42)
# wc3ëŠ” wc1ê³¼ ë‹¤ë¥¸ ë°°ì¹˜ (ì‹œë“œ ê°’ì´ ë‹¤ë¦„)
```

**ì™œ ëœë¤ ì‹œë“œë¥¼ ê³ ì •í•˜ë‚˜?**
- ì›Œë“œí´ë¼ìš°ë“œëŠ” ë‹¨ì–´ ë°°ì¹˜ê°€ ë¬´ì‘ìœ„
- ê°™ì€ ë°ì´í„°ë¡œ ìƒì„±í•´ë„ ë§¤ë²ˆ ë‹¤ë¥¸ ëª¨ì–‘
- `random_state`ë¥¼ ê³ ì •í•˜ë©´ ë™ì¼í•œ ê²°ê³¼ ì¬í˜„ ê°€ëŠ¥
- ë””ë²„ê¹…, í…ŒìŠ¤íŠ¸, ë¹„êµ ë¶„ì„ì— ìœ ìš©

**4. `show`:**
```python
# API ì„œë²„ì—ì„œëŠ” show=Falseë¡œ ì„¤ì •
wc = nlp.generate_wordcloud(fd, show=False)  # ê·¸ë˜í”„ ì°½ ë„ìš°ì§€ ì•ŠìŒ

# ë¡œì»¬ í…ŒìŠ¤íŠ¸ì—ì„œëŠ” show=True
wc = nlp.generate_wordcloud(fd, show=True)   # Matplotlib ì°½ì— í‘œì‹œ
```

#### ğŸ” ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì›ë¦¬

**1. ë¹ˆë„ ë°ì´í„° â†’ í¬ê¸° ë§¤í•‘:**

```
ë¹ˆë„ ë¶„í¬:
Emma: 191íšŒ
Knightley: 115íšŒ
Frank: 89íšŒ
...

â†“ WordCloud ì•Œê³ ë¦¬ì¦˜

ì‹œê°í™”:
Emma (ê°€ì¥ í¼)
Knightley (ì¤‘ê°„)
Frank (ì‘ìŒ)
```

**2. ë ˆì´ì•„ì›ƒ ì•Œê³ ë¦¬ì¦˜:**
- ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ë¥¼ ì¤‘ì•™ì— ë°°ì¹˜
- ë‚®ì€ ë¹ˆë„ ë‹¨ì–´ëŠ” ì£¼ë³€ì— ë°°ì¹˜
- ê²¹ì¹˜ì§€ ì•Šë„ë¡ ìœ„ì¹˜ ê³„ì‚°
- ê³µê°„ íš¨ìœ¨ì ìœ¼ë¡œ ì±„ìš°ê¸°

**3. ìƒ‰ìƒ ì„ íƒ:**
```python
# ê¸°ë³¸ ìƒ‰ìƒ íŒ”ë ˆíŠ¸ (íŒŒë€ìƒ‰ ê³„ì—´)
wc = WordCloud(colormap='viridis')

# ë‹¤ì–‘í•œ ìƒ‰ìƒ ì˜µì…˜
wc = WordCloud(colormap='rainbow')  # ë¬´ì§€ê°œ
wc = WordCloud(colormap='hot')      # ë¹¨ê°•-ë…¸ë‘
wc = WordCloud(colormap='cool')     # íŒŒë‘-ë³´ë¼
```

#### âœ… ìë™ ì €ì¥ ê¸°ëŠ¥

```python
# ìë™ ì €ì¥ ì½”ë“œ ë¶„ì„
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
# ê²°ê³¼: "20251211_092138"

filename = f"emma_wordcloud_{timestamp}.png"
# ê²°ê³¼: "emma_wordcloud_20251211_092138.png"

filepath = self.save_dir / filename
# ê²°ê³¼: Path('app/nlp/save/emma_wordcloud_20251211_092138.png')

wc.to_file(str(filepath))
# WordCloudë¥¼ PNG íŒŒì¼ë¡œ ì €ì¥
```

**íŒŒì¼ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•˜ëŠ” ì´ìœ :**
- ê°™ì€ ì´ë¦„ìœ¼ë¡œ ë®ì–´ì“°ì§€ ì•ŠìŒ
- ì´ë ¥ ê´€ë¦¬ (ë²„ì „ë³„ ë¹„êµ ê°€ëŠ¥)
- ì–¸ì œ ìƒì„±ë˜ì—ˆëŠ”ì§€ ëª…í™•íˆ ì•Œ ìˆ˜ ìˆìŒ
- ë™ì‹œì— ì—¬ëŸ¬ ìš”ì²­ì´ ì™€ë„ ì¶©ëŒ ì—†ìŒ

---

### Step 4: FastAPI ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„

#### ğŸ¯ ëª©í‘œ
NLPService í´ë˜ìŠ¤ë¥¼ FastAPIë¡œ ì›¹ ì„œë¹„ìŠ¤í™”í•˜ì—¬ HTTP APIë¡œ ì œê³µ

#### ğŸ’» API ë¼ìš°í„° êµ¬í˜„

**`nlp_router.py` ì „ì²´ êµ¬ì¡°:**

```python
from fastapi import APIRouter, HTTPException, Query
from fastapi.responses import Response
from io import BytesIO
from app.nlp.emma.emma_wordcloud import NLPService

# ========================================
# 1. ë¼ìš°í„° ìƒì„± ë° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
# ========================================
router = APIRouter(prefix="/nlp", tags=["nlp"])
nlp_service = NLPService(download_nltk_data=True)


# ========================================
# 2. ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì—”ë“œí¬ì¸íŠ¸
# ========================================
@router.get("/emma")
async def generate_emma_wordcloud(
    width: int = Query(1000, description="ì´ë¯¸ì§€ ë„ˆë¹„"),
    height: int = Query(600, description="ì´ë¯¸ì§€ ë†’ì´"),
    background_color: str = Query("white", description="ë°°ê²½ìƒ‰"),
    random_state: int = Query(0, description="ëœë¤ ì‹œë“œ")
):
    """
    Emma ë§ë­‰ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    
    **ì²˜ë¦¬ ê³¼ì •:**
    1. Emma ë§ë­‰ì¹˜ ë¡œë“œ
    2. ì •ê·œì‹ í† í°í™”
    3. POS íƒœê¹…
    4. ê³ ìœ ëª…ì‚¬(NNP) í•„í„°ë§
    5. ë¹ˆë„ ë¶„ì„
    6. ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    7. PNG ì´ë¯¸ì§€ ë°˜í™˜
    
    **Query íŒŒë¼ë¯¸í„°:**
    - width: ì´ë¯¸ì§€ ë„ˆë¹„ (ê¸°ë³¸ê°’: 1000)
    - height: ì´ë¯¸ì§€ ë†’ì´ (ê¸°ë³¸ê°’: 600)
    - background_color: ë°°ê²½ìƒ‰ (ê¸°ë³¸ê°’: "white")
    - random_state: ëœë¤ ì‹œë“œ (ê¸°ë³¸ê°’: 0)
    
    **ë°˜í™˜:**
    - PNG í˜•ì‹ì˜ ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€
    """
    try:
        # Step 1: Emma ë§ë­‰ì¹˜ ë¡œë“œ
        emma_raw = nlp_service.load_corpus("austen-emma.txt")
        
        # Step 2: í† í°í™” (ì •ê·œì‹ ì‚¬ìš©)
        emma_tokens = nlp_service.tokenize_regex(emma_raw)
        
        # Step 3: POS íƒœê¹…
        tagged_tokens = nlp_service.pos_tag(emma_tokens)
        
        # Step 4: ê³ ìœ ëª…ì‚¬(NNP)ë§Œ í•„í„°ë§
        stopwords = ["Mr.", "Mrs.", "Miss", "Mr", "Mrs", "Dear"]
        names = nlp_service.filter_tokens_by_pos(
            pos_tag="NNP",
            stopwords=stopwords,
            tagged_list=tagged_tokens
        )
        
        # Step 5: ë¹ˆë„ ë¶„í¬ ìƒì„±
        fd_names = nlp_service.create_freq_dist(names)
        
        # Step 6: ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± (show=False)
        wc = nlp_service.generate_wordcloud(
            freq_dist=fd_names,
            width=width,
            height=height,
            background_color=background_color,
            random_state=random_state,
            show=False  # API ì„œë²„ì—ì„œëŠ” ê·¸ë˜í”„ ì°½ ë„ìš°ì§€ ì•ŠìŒ
        )
        
        # Step 7: ì´ë¯¸ì§€ë¥¼ BytesIOë¡œ ë³€í™˜
        from PIL import Image
        import numpy as np
        
        # WordCloudë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        img_array = wc.to_array()
        
        # numpy ë°°ì—´ì„ PIL Imageë¡œ ë³€í™˜
        img = Image.fromarray(img_array)
        
        # PIL Imageë¥¼ BytesIO ë²„í¼ì— ì €ì¥
        img_buffer = BytesIO()
        img.save(img_buffer, format='PNG')
        img_buffer.seek(0)
        
        # Step 8: Responseë¡œ ë°˜í™˜
        return Response(
            content=img_buffer.getvalue(),
            media_type="image/png",
            headers={
                "Content-Disposition": "inline; filename=emma_wordcloud.png"
            }
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨: {str(e)}"
        )
```

#### ğŸ“ ì½”ë“œ ìƒì„¸ ë¶„ì„

**1. ë¼ìš°í„° ìƒì„±:**

```python
router = APIRouter(prefix="/nlp", tags=["nlp"])
```

- `prefix="/nlp"`: ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ì— `/nlp` ì ‘ë‘ì‚¬ ì¶”ê°€
- `tags=["nlp"]`: Swagger ë¬¸ì„œí™” ì‹œ ê·¸ë£¹ ë¶„ë¥˜
- ì‹¤ì œ ê²½ë¡œ: `/nlp/emma`, `/nlp/emma/stats`

**2. ì„œë¹„ìŠ¤ ì´ˆê¸°í™”:**

```python
nlp_service = NLPService(download_nltk_data=True)
```

- ëª¨ë“ˆ ë¡œë“œ ì‹œ í•œ ë²ˆë§Œ ì´ˆê¸°í™”
- ëª¨ë“  ìš”ì²­ì´ ê°™ì€ ì¸ìŠ¤í„´ìŠ¤ ê³µìœ 
- NLTK ë°ì´í„°ëŠ” ìµœì´ˆ í•œ ë²ˆë§Œ ë‹¤ìš´ë¡œë“œ

**3. Query íŒŒë¼ë¯¸í„°:**

```python
width: int = Query(1000, description="ì´ë¯¸ì§€ ë„ˆë¹„")
```

- `Query()`: FastAPIì˜ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ê²€ì¦
- ê¸°ë³¸ê°’: 1000
- `description`: Swagger ë¬¸ì„œì— í‘œì‹œë  ì„¤ëª…
- ìë™ íƒ€ì… ë³€í™˜: `"1000"` (ë¬¸ìì—´) â†’ `1000` (ì •ìˆ˜)

**URL ì˜ˆì‹œ:**
```
GET /nlp/emma
GET /nlp/emma?width=1920&height=1080
GET /nlp/emma?background_color=black&random_state=42
```

**4. NLP íŒŒì´í”„ë¼ì¸:**

```python
# ì²´ì´ë‹ ë°©ì‹ìœ¼ë¡œ ë‹¨ê³„ë³„ ì²˜ë¦¬
emma_raw = nlp_service.load_corpus("austen-emma.txt")
emma_tokens = nlp_service.tokenize_regex(emma_raw)
tagged_tokens = nlp_service.pos_tag(emma_tokens)
names = nlp_service.filter_tokens_by_pos("NNP", stopwords, tagged_tokens)
fd_names = nlp_service.create_freq_dist(names)
wc = nlp_service.generate_wordcloud(fd_names, ...)
```

**ê° ë‹¨ê³„ë³„ ë°ì´í„° ë³€í™˜:**

```
Step 1: load_corpus
â†’ "[Emma by Jane Austen 1816]..."  (ê¸´ ë¬¸ìì—´)

Step 2: tokenize_regex
â†’ ["Emma", "by", "Jane", "Austen", ...]  (í† í° ë¦¬ìŠ¤íŠ¸)

Step 3: pos_tag
â†’ [("Emma", "NNP"), ("by", "IN"), ("Jane", "NNP"), ...]  (íŠœí”Œ ë¦¬ìŠ¤íŠ¸)

Step 4: filter_tokens_by_pos
â†’ ["Emma", "Jane", "Austen", "Knightley", ...]  (ê³ ìœ ëª…ì‚¬ë§Œ)

Step 5: create_freq_dist
â†’ FreqDist({'Emma': 191, 'Knightley': 115, ...})  (ë¹ˆë„ ì‚¬ì „)

Step 6: generate_wordcloud
â†’ WordCloud ê°ì²´ (ì´ë¯¸ì§€ ë°ì´í„°)
```

**5. ì´ë¯¸ì§€ ë³€í™˜ ê³¼ì •:**

```python
# WordCloud â†’ numpy array
img_array = wc.to_array()
# í˜•íƒœ: (600, 1000, 3) RGB ì´ë¯¸ì§€

# numpy array â†’ PIL Image
img = Image.fromarray(img_array)

# PIL Image â†’ BytesIO
img_buffer = BytesIO()
img.save(img_buffer, format='PNG')
img_buffer.seek(0)
```

**ì™œ ì´ë ‡ê²Œ ë³µì¡í•œ ë³€í™˜ì´ í•„ìš”í•œê°€?**
- WordCloudëŠ” numpy ë°°ì—´ë¡œ ë°ì´í„° ì €ì¥
- FastAPI ResponseëŠ” bytes íƒ€ì… ìš”êµ¬
- PILì€ numpy â†” bytes ë³€í™˜ì˜ ì¤‘ê°œì
- BytesIOëŠ” ë©”ëª¨ë¦¬ ìƒì˜ íŒŒì¼ ê°ì²´ (ë””ìŠ¤í¬ I/O ì—†ìŒ)

**6. Response ìƒì„±:**

```python
return Response(
    content=img_buffer.getvalue(),  # bytes ë°ì´í„°
    media_type="image/png",          # MIME íƒ€ì…
    headers={
        "Content-Disposition": "inline; filename=emma_wordcloud.png"
    }
)
```

**Response íŒŒë¼ë¯¸í„° ì„¤ëª…:**
- `content`: ì‹¤ì œ ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë°ì´í„°
- `media_type`: ë¸Œë¼ìš°ì €ì—ê²Œ ì´ê²ƒì´ PNG ì´ë¯¸ì§€ì„ì„ ì•Œë¦¼
- `Content-Disposition: inline`: ë‹¤ìš´ë¡œë“œ ëŒ€ì‹  ë¸Œë¼ìš°ì €ì—ì„œ í‘œì‹œ
- `filename`: ì €ì¥ ì‹œ ê¸°ë³¸ íŒŒì¼ëª…

#### ğŸ’» í†µê³„ ì •ë³´ ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸

```python
@router.get("/emma/stats")
async def get_emma_stats():
    """
    Emma ë§ë­‰ì¹˜ í†µê³„ ì •ë³´ ì¡°íšŒ
    
    **ë°˜í™˜ ë°ì´í„°:**
    - ì „ì²´ ë‹¨ì–´ ìˆ˜
    - ê°€ì¥ ë¹ˆë„ ë†’ì€ ë‹¨ì–´ Top 10
    - "Emma" ë‹¨ì–´ì˜ ì¶œí˜„ íšŸìˆ˜ ë° í™•ë¥ 
    """
    try:
        # 1~5ë‹¨ê³„: ì›Œë“œí´ë¼ìš°ë“œì™€ ë™ì¼í•œ ì „ì²˜ë¦¬
        emma_raw = nlp_service.load_corpus("austen-emma.txt")
        emma_tokens = nlp_service.tokenize_regex(emma_raw)
        tagged_tokens = nlp_service.pos_tag(emma_tokens)
        
        stopwords = ["Mr.", "Mrs.", "Miss", "Mr", "Mrs", "Dear"]
        names = nlp_service.filter_tokens_by_pos(
            pos_tag="NNP",
            stopwords=stopwords,
            tagged_list=tagged_tokens
        )
        
        fd_names = nlp_service.create_freq_dist(names)
        
        # 6. í†µê³„ ì •ë³´ ìˆ˜ì§‘
        total, emma_count, emma_freq = nlp_service.get_freq_statistics(fd_names, "Emma")
        most_common = nlp_service.get_most_common(fd_names, 10)
        
        # 7. JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
        return {
            "status": "success",
            "total_words": total,
            "emma_count": emma_count,
            "emma_frequency": round(emma_freq, 4),
            "top_10_words": [
                {"word": word, "count": count} 
                for word, count in most_common
            ]
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        )
```

**ë°˜í™˜ ë°ì´í„° ì˜ˆì‹œ:**

```json
{
  "status": "success",
  "total_words": 865,
  "emma_count": 191,
  "emma_frequency": 0.2208,
  "top_10_words": [
    {"word": "Emma", "count": 191},
    {"word": "Knightley", "count": 115},
    {"word": "Frank", "count": 89},
    {"word": "Harriet", "count": 73},
    {"word": "Jane", "count": 61},
    {"word": "Weston", "count": 52},
    {"word": "Elton", "count": 49},
    {"word": "Miss", "count": 38},
    {"word": "Churchill", "count": 35},
    {"word": "Fairfax", "count": 31}
  ]
}
```

#### ğŸ” API ìš”ì²­-ì‘ë‹µ íë¦„

```
1. ì‚¬ìš©ì: ë¸Œë¼ìš°ì €ì—ì„œ URL ì…ë ¥
   GET http://localhost:8080/api/ml/nlp/emma?width=1920

2. API Gateway: ê²½ë¡œ ì¬ì‘ì„±
   /api/ml/nlp/emma â†’ /nlp/emma

3. mlservice (Docker): FastAPI ì„œë²„
   nlp_router.py: generate_emma_wordcloud() í˜¸ì¶œ

4. NLPService: ìì—°ì–´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
   â”œâ”€ load_corpus() â†’ Emma í…ìŠ¤íŠ¸ ë¡œë“œ
   â”œâ”€ tokenize_regex() â†’ í† í°í™”
   â”œâ”€ pos_tag() â†’ í’ˆì‚¬ íƒœê¹…
   â”œâ”€ filter_tokens_by_pos() â†’ ê³ ìœ ëª…ì‚¬ í•„í„°ë§
   â”œâ”€ create_freq_dist() â†’ ë¹ˆë„ ë¶„ì„
   â””â”€ generate_wordcloud() â†’ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
       â””â”€ save to app/nlp/save/emma_wordcloud_*.png

5. FastAPI: Image â†’ BytesIO â†’ Response
   Content-Type: image/png

6. ë¸Œë¼ìš°ì €: PNG ì´ë¯¸ì§€ ìˆ˜ì‹  ë° í‘œì‹œ
   â””â”€ ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™”
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨

#### ì¦ìƒ
```python
LookupError: 
**********************************************
  Resource 'corpora/gutenberg' not found.
  Please use the NLTK Downloader to obtain the resource:
  >>> import nltk
  >>> nltk.download('gutenberg')
**********************************************
```

#### ì›ì¸ ë¶„ì„
- NLTKëŠ” íŒ¨í‚¤ì§€ ì„¤ì¹˜ë§Œìœ¼ë¡œëŠ” ì‚¬ìš© ë¶ˆê°€
- ë³„ë„ë¡œ ë§ë­‰ì¹˜, íƒœê±° ë“±ì˜ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•´ì•¼ í•¨
- Docker ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘ ì‹œ ë°ì´í„° ì†ì‹¤

#### í•´ê²° ê³¼ì •

**1ë‹¨ê³„: ì´ˆê¸°í™” ì‹œ ìë™ ë‹¤ìš´ë¡œë“œ**
```python
def __init__(self, download_nltk_data: bool = True):
    if download_nltk_data:
        nltk.download('book', quiet=True)
        nltk.download('punkt', quiet=True)
        nltk.download('wordnet', quiet=True)
        nltk.download('averaged_perceptron_tagger', quiet=True)
        nltk.download('omw-1.4', quiet=True)
```

**2ë‹¨ê³„: Docker ë³¼ë¥¨ ë§ˆìš´íŠ¸ë¡œ ì˜ì†í™”**

```yaml
# docker-compose.yaml
mlservice:
  volumes:
    - nltk_data:/root/nltk_data  # NLTK ë°ì´í„° ì˜ì†í™”

volumes:
  nltk_data:  # Named volume ì •ì˜
```

**3ë‹¨ê³„: ì—ëŸ¬ ì²˜ë¦¬ ì¶”ê°€**
```python
try:
    nltk.download('averaged_perceptron_tagger_eng', quiet=True)
except:
    pass  # ì¼ë¶€ ë²„ì „ì—ì„œëŠ” í•„ìš” ì—†ì„ ìˆ˜ ìˆìŒ
```

#### ê²€ì¦ ë°©ë²•
```python
# Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ í™•ì¸
docker compose exec mlservice python -c "
import nltk
print(nltk.data.path)
print(nltk.corpus.gutenberg.fileids())
"
```

#### ìµœì¢… í•´ê²°ì±…
- ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œ ìë™ ë‹¤ìš´ë¡œë“œ
- Docker ë³¼ë¥¨ìœ¼ë¡œ ë°ì´í„° ì˜ì†í™”
- ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œì—ë„ í”„ë¡œê·¸ë¨ ê³„ì† ì‹¤í–‰

---

### ë¬¸ì œ 2: "Resource 'averaged_perceptron_tagger_eng' not found"

#### ì¦ìƒ
```
500 Internal Server Error
Resource 'averaged_perceptron_tagger_eng' not found.
```

#### ì›ì¸ ë¶„ì„
- NLTK 3.8+ ë²„ì „ì—ì„œ POS íƒœê±° ë¦¬ì†ŒìŠ¤ ì´ë¦„ ë³€ê²½
- `averaged_perceptron_tagger_eng`ê°€ í•„ìš”í•œ ê²½ìš° ë°œìƒ
- ì¼ë¶€ í™˜ê²½ì—ì„œëŠ” ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë˜ì§€ ì•ŠìŒ

#### í•´ê²° ê³¼ì •

**ì‹œë„ 1: ì¶”ê°€ ë‹¤ìš´ë¡œë“œ**
```python
nltk.download('averaged_perceptron_tagger_eng', quiet=True)
```

**ì‹œë„ 2: ì˜ˆì™¸ ì²˜ë¦¬**
```python
try:
    nltk.download('averaged_perceptron_tagger_eng', quiet=True)
except:
    pass  # ë²„ì „ì— ë”°ë¼ í•„ìš” ì—†ì„ ìˆ˜ ìˆìŒ
```

**ì‹œë„ 3: Universal Tagset ë‹¤ìš´ë¡œë“œ**
```python
nltk.download('universal_tagset', quiet=True)
```

#### ìµœì¢… í•´ê²°ì±…
```python
def __init__(self, download_nltk_data: bool = True):
    if download_nltk_data:
        try:
            nltk.download('book', quiet=True)
            nltk.download('punkt', quiet=True)
            nltk.download('wordnet', quiet=True)
            nltk.download('averaged_perceptron_tagger', quiet=True)
            # ì¶”ê°€: ëª¨ë“  ê´€ë ¨ íƒœê±° ë°ì´í„°
            try:
                nltk.download('averaged_perceptron_tagger_eng', quiet=True)
            except:
                pass
            nltk.download('universal_tagset', quiet=True)
            nltk.download('omw-1.4', quiet=True)
        except Exception as e:
            print(f"NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
```

---

### ë¬¸ì œ 3: API Gatewayì—ì„œ 404 Not Found

#### ì¦ìƒ
```
GET http://localhost:8080/api/ml/nlp/emma
â†’ 404 Not Found
```

#### ì›ì¸ ë¶„ì„
- API Gatewayì˜ `application.yaml`ì— `/nlp` ê²½ë¡œ ë¼ìš°íŒ… ì„¤ì • ëˆ„ë½
- ê¸°ì¡´ ì„¤ì •ì—ëŠ” `/titanic`, `/usa`, `/seoul` ê²½ë¡œë§Œ ì¡´ì¬

#### í•´ê²° ê³¼ì •

**1ë‹¨ê³„: ë¼ìš°íŒ… ê·œì¹™ í™•ì¸**
```yaml
# application.yaml í™•ì¸
routes:
  - id: titanic-service-route
    predicates:
      - Path=/api/ml/titanic/**
    filters:
      - RewritePath=/api/ml/titanic/(?<segment>.*), /titanic/${segment}
  
  # NLP ë¼ìš°íŒ… ì—†ìŒ!
```

**2ë‹¨ê³„: NLP ë¼ìš°íŒ… ì¶”ê°€**
```yaml
# application.yamlì— ì¶”ê°€
- id: nlp-service-route
  uri: http://mlservice:9006
  predicates:
    - Path=/api/ml/nlp/**
  filters:
    - RewritePath=/api/ml/nlp/(?<segment>.*), /nlp/${segment}
```

**3ë‹¨ê³„: Gateway ì¬ì‹œì‘**
```bash
docker compose restart api-gateway
```

#### ê²€ì¦ ë°©ë²•
```bash
# ì§ì ‘ mlservice ì ‘ê·¼ (Gateway ìš°íšŒ)
curl http://localhost:9006/nlp/emma
# â†’ 200 OK

# Gatewayë¥¼ í†µí•œ ì ‘ê·¼
curl http://localhost:8080/api/ml/nlp/emma
# â†’ 200 OK
```

#### ìµœì¢… í•´ê²°ì±…
- API Gatewayì— NLP ì„œë¹„ìŠ¤ ë¼ìš°íŒ… ê·œì¹™ ì¶”ê°€
- ê²½ë¡œ ì¬ì‘ì„± íŒ¨í„´: `/api/ml/nlp/**` â†’ `/nlp/**`
- ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘ìœ¼ë¡œ ì„¤ì • ì ìš©

---

### ë¬¸ì œ 4: ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€ê°€ save í´ë”ì— ì €ì¥ë˜ì§€ ì•ŠìŒ

#### ì¦ìƒ
- API í˜¸ì¶œ ì„±ê³µ (200 OK)
- ë¸Œë¼ìš°ì €ì— ì›Œë“œí´ë¼ìš°ë“œ í‘œì‹œë¨
- `app/nlp/save/` í´ë”ì— íŒŒì¼ ì—†ìŒ

#### ì›ì¸ ë¶„ì„
- `generate_wordcloud()` ë©”ì„œë“œì— ì €ì¥ ë¡œì§ ëˆ„ë½
- ì´ë¯¸ì§€ê°€ ë©”ëª¨ë¦¬ì—ë§Œ ìƒì„±ë˜ê³  ë””ìŠ¤í¬ì— ì €ì¥ë˜ì§€ ì•ŠìŒ

#### í•´ê²° ê³¼ì •

**1ë‹¨ê³„: ì €ì¥ ë¡œì§ ì¶”ê°€**
```python
def generate_wordcloud(self, freq_dist, ...):
    wc = WordCloud(...)
    wc.generate_from_frequencies(freq_dist)
    
    # ì €ì¥ ë¡œì§ ì¶”ê°€
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"emma_wordcloud_{timestamp}.png"
    filepath = self.save_dir / filename
    wc.to_file(str(filepath))  # â† ì´ ë¶€ë¶„ ì¶”ê°€
    
    return wc
```

**2ë‹¨ê³„: save_dir ê²½ë¡œ í™•ì¸**
```python
def __init__(self):
    # save í´ë” ê²½ë¡œ ì„¤ì •
    self.save_dir = Path(__file__).parent.parent / 'save'
    # emma_wordcloud.py â†’ emma/ â†’ nlp/ â†’ nlp/save/
    
    # í´ë” ìƒì„±
    self.save_dir.mkdir(exist_ok=True)
```

**3ë‹¨ê³„: Docker ë³¼ë¥¨ ë§ˆìš´íŠ¸ ì¶”ê°€**
```yaml
# docker-compose.yaml
mlservice:
  volumes:
    - ./ai.kroaddy.site/services/mlservice/app/nlp/save:/app/app/nlp/save
```

#### ê²€ì¦ ë°©ë²•
```bash
# API í˜¸ì¶œ í›„ íŒŒì¼ í™•ì¸
ls ai.kroaddy.site/services/mlservice/app/nlp/save/
# emma_wordcloud_20251211_092138.png

# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ í™•ì¸
docker compose exec mlservice ls -la /app/app/nlp/save/
```

#### ìµœì¢… í•´ê²°ì±…
- `generate_wordcloud()` ë©”ì„œë“œì— ìë™ ì €ì¥ ì¶”ê°€
- Docker ë³¼ë¥¨ ë§ˆìš´íŠ¸ë¡œ í˜¸ìŠ¤íŠ¸ì—ì„œ íŒŒì¼ ì ‘ê·¼ ê°€ëŠ¥
- íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ íŒŒì¼ëª…ìœ¼ë¡œ ì´ë ¥ ê´€ë¦¬

---

## ìµœì¢… ê²°ê³¼ë¬¼

### API ì—”ë“œí¬ì¸íŠ¸ ëª©ë¡

| ë©”ì„œë“œ | ì—”ë“œí¬ì¸íŠ¸ | ì„¤ëª… | ë°˜í™˜ í˜•ì‹ |
|--------|-----------|------|----------|
| GET | `/nlp/emma` | Emma ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± | image/png |
| GET | `/nlp/emma/stats` | Emma í†µê³„ ì •ë³´ ì¡°íšŒ | application/json |

**API Gatewayë¥¼ í†µí•œ ì ‘ê·¼:**
- `http://localhost:8080/api/ml/nlp/emma`
- `http://localhost:8080/api/ml/nlp/emma/stats`

**ì§ì ‘ mlservice ì ‘ê·¼:**
- `http://localhost:9006/nlp/emma`
- `http://localhost:9006/nlp/emma/stats`

### ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ

**1. ê¸°ë³¸ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±:**
```
GET http://localhost:8080/api/ml/nlp/emma
â†’ 1000x600 PNG ì´ë¯¸ì§€ ë°˜í™˜
```

**2. ì»¤ìŠ¤í„°ë§ˆì´ì§•:**
```
GET http://localhost:8080/api/ml/nlp/emma?width=1920&height=1080&background_color=black&random_state=42
â†’ Full HD í¬ê¸°, ê²€ì€ ë°°ê²½, ì‹œë“œ 42
```

**3. í†µê³„ ì •ë³´ ì¡°íšŒ:**
```
GET http://localhost:8080/api/ml/nlp/emma/stats
â†’ JSON ì‘ë‹µ
```

**ì‘ë‹µ ì˜ˆì‹œ:**
```json
{
  "status": "success",
  "total_words": 865,
  "emma_count": 191,
  "emma_frequency": 0.2208,
  "top_10_words": [
    {"word": "Emma", "count": 191},
    {"word": "Knightley", "count": 115},
    {"word": "Frank", "count": 89},
    ...
  ]
}
```

### ì €ì¥ëœ íŒŒì¼ êµ¬ì¡°

```
app/nlp/
â”œâ”€â”€ emma/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ emma_wordcloud.py (NLPService í´ë˜ìŠ¤, 766ì¤„)
â”œâ”€â”€ save/
â”‚   â””â”€â”€ emma_wordcloud_20251211_092138.png (ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ D2Coding.ttf
â”‚   â”œâ”€â”€ kr-Report_2018.txt
â”‚   â””â”€â”€ stopwords.txt
â”œâ”€â”€ __init__.py
â””â”€â”€ nlp_router.py (API ì—”ë“œí¬ì¸íŠ¸, 146ì¤„)
```

### NLPService í´ë˜ìŠ¤ ë©”ì„œë“œ ëª©ë¡

**ë§ë­‰ì¹˜ ê´€ë¦¬ (2ê°œ):**
- `get_corpus_fileids()`: ë§ë­‰ì¹˜ íŒŒì¼ ëª©ë¡
- `load_corpus()`: ë§ë­‰ì¹˜ ë¡œë“œ

**í† í°í™” (3ê°œ):**
- `tokenize_sentences()`: ë¬¸ì¥ ë‹¨ìœ„
- `tokenize_words()`: ë‹¨ì–´ ë‹¨ìœ„
- `tokenize_regex()`: ì •ê·œì‹ ê¸°ë°˜

**í˜•íƒœì†Œ ë¶„ì„ (4ê°œ):**
- `stem_porter()`: Porter Stemmer
- `stem_lancaster()`: Lancaster Stemmer
- `lemmatize()`: ì›í˜• ë³µì›
- `lemmatize_word()`: ë‹¨ì¼ ë‹¨ì–´ ì›í˜• ë³µì›

**í’ˆì‚¬ íƒœê¹… (5ê°œ):**
- `get_pos_tag_info()`: íƒœê·¸ ì •ë³´
- `pos_tag()`: í’ˆì‚¬ íƒœê¹…
- `extract_nouns()`: ëª…ì‚¬ ì¶”ì¶œ
- `remove_tags()`: íƒœê·¸ ì œê±°
- `create_pos_tokenizer()`: POS í¬í•¨ í† í°

**í…ìŠ¤íŠ¸ ë¶„ì„ (6ê°œ):**
- `create_text_analyzer()`: Text ê°ì²´ ìƒì„±
- `plot_word_frequency()`: ë¹ˆë„ ê·¸ë˜í”„
- `plot_dispersion()`: ë¶„ì‚° í”Œë¡¯
- `find_concordance()`: ë‹¨ì–´ ìœ„ì¹˜
- `find_similar_words()`: ìœ ì‚¬ ë‹¨ì–´
- `find_collocations()`: ì—°ì–´ ì°¾ê¸°

**ë¹ˆë„ ë¶„ì„ (4ê°œ):**
- `get_vocabulary()`: ì–´íœ˜ ë¹ˆë„
- `create_freq_dist()`: ë¹ˆë„ ë¶„í¬
- `filter_tokens_by_pos()`: í’ˆì‚¬ í•„í„°ë§
- `get_freq_statistics()`: í†µê³„ ì •ë³´
- `get_most_common()`: Top N ë‹¨ì–´

**ì›Œë“œí´ë¼ìš°ë“œ (2ê°œ):**
- `generate_wordcloud()`: ìƒì„± ë° ì €ì¥
- `save_wordcloud()`: íŒŒì¼ ì €ì¥

**ì´ 26ê°œ ë©”ì„œë“œ**

---

## í•™ìŠµ ì •ë¦¬

### ë°°ìš´ ê¸°ìˆ  ìŠ¤í‚¬

#### 1. NLTK ìì—°ì–´ ì²˜ë¦¬

**í•µì‹¬ ê°œë…:**
- **ë§ë­‰ì¹˜(Corpus)**: ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë°ì´í„° ì§‘í•©
- **í† í°í™”(Tokenization)**: ë¬¸ìì—´ì„ ë¶„ì„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
- **í˜•íƒœì†Œ ë¶„ì„(Morphology)**: ë‹¨ì–´ì˜ ì–´ê·¼, ì ‘ì‚¬ ë¶„ì„
- **í’ˆì‚¬ íƒœê¹…(POS Tagging)**: ë‹¨ì–´ì˜ ë¬¸ë²•ì  ì—­í•  íŒŒì•…
- **ë¹ˆë„ ë¶„ì„(Frequency)**: ë‹¨ì–´ ì¶œí˜„ íšŸìˆ˜ ê³„ì‚°

**ì‹¤ë¬´ í™œìš©:**
- ê²€ìƒ‰ ì—”ì§„: ì¿¼ë¦¬ ë¶„ì„ ë° ë¬¸ì„œ ì¸ë±ì‹±
- ì±—ë´‡: ì‚¬ìš©ì ì˜ë„ íŒŒì•…
- ê°ì • ë¶„ì„: ê¸ì •/ë¶€ì • íŒë‹¨
- ë¬¸ì„œ ìš”ì•½: í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ

#### 2. ê°ì²´ì§€í–¥ í”„ë¡œê·¸ë˜ë°(OOP)

**í•µì‹¬ ê°œë…:**
- **ìº¡ìŠí™”(Encapsulation)**: ë°ì´í„°ì™€ ë©”ì„œë“œë¥¼ í•˜ë‚˜ë¡œ ë¬¶ìŒ
- **ì¶”ìƒí™”(Abstraction)**: ë³µì¡í•œ ë¡œì§ì„ ê°„ë‹¨í•œ ì¸í„°í˜ì´ìŠ¤ë¡œ
- **ì¬ì‚¬ìš©ì„±(Reusability)**: í•œ ë²ˆ ì‘ì„±í•œ ì½”ë“œë¥¼ ì—¬ëŸ¬ ê³³ì—ì„œ ì‚¬ìš©
- **ìœ ì§€ë³´ìˆ˜ì„±(Maintainability)**: ìˆ˜ì •ì´ ì‰½ê³  ì˜í–¥ ë²”ìœ„ê°€ ëª…í™•

**ì‹¤ë¬´ í™œìš©:**
- ë¼ì´ë¸ŒëŸ¬ë¦¬/í”„ë ˆì„ì›Œí¬ ì„¤ê³„
- ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸ ì½”ë“œ êµ¬ì¡°í™”
- íŒ€ í˜‘ì—… (ëª…í™•í•œ ì¸í„°í˜ì´ìŠ¤)
- í…ŒìŠ¤íŠ¸ ì£¼ë„ ê°œë°œ(TDD)

#### 3. FastAPI ì´ë¯¸ì§€ ì‘ë‹µ

**í•µì‹¬ ê°œë…:**
- **Response í´ë˜ìŠ¤**: ì»¤ìŠ¤í…€ ì‘ë‹µ ìƒì„±
- **BytesIO**: ë©”ëª¨ë¦¬ ìƒì˜ íŒŒì¼ ê°ì²´
- **PIL (Pillow)**: ì´ë¯¸ì§€ í¬ë§· ë³€í™˜
- **MIME íƒ€ì…**: ë¸Œë¼ìš°ì €ì—ê²Œ ì½˜í…ì¸  íƒ€ì… ì•Œë¦¼

**ì‹¤ë¬´ í™œìš©:**
- ë™ì  ì´ë¯¸ì§€ ìƒì„± (ì°¨íŠ¸, ê·¸ë˜í”„)
- ì¸ë„¤ì¼ ìƒì„±
- ì´ë¯¸ì§€ í•„í„° ì ìš©
- PDF ìƒì„± ë° ì „ì†¡

#### 4. Docker ë°ì´í„° ì˜ì†í™”

**í•µì‹¬ ê°œë…:**
- **Named Volume**: Dockerê°€ ê´€ë¦¬í•˜ëŠ” ì˜êµ¬ ì €ì¥ì†Œ
- **Bind Mount**: í˜¸ìŠ¤íŠ¸ ê²½ë¡œë¥¼ ì»¨í…Œì´ë„ˆì— ë§ˆìš´íŠ¸
- **ë°ì´í„° ë¼ì´í”„ì‚¬ì´í´**: ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘í•´ë„ ë°ì´í„° ë³´ì¡´

**ì‹¤ë¬´ í™œìš©:**
- ë°ì´í„°ë² ì´ìŠ¤ ë°ì´í„° ë³´ì¡´
- ë¡œê·¸ íŒŒì¼ ì €ì¥
- ë¹Œë“œ ìºì‹œ ê´€ë¦¬
- ê°œë°œ í™˜ê²½ê³¼ ìš´ì˜ í™˜ê²½ ë™ê¸°í™”

#### 5. ì •ê·œì‹(Regular Expression)

**í•µì‹¬ ê°œë…:**
- **íŒ¨í„´ ë§¤ì¹­**: ë¬¸ìì—´ì—ì„œ íŠ¹ì • íŒ¨í„´ ì°¾ê¸°
- **í† í°í™”**: ë‹¨ì–´, ìˆ«ì, ê¸°í˜¸ ë“± ë¶„ë¦¬
- **í…ìŠ¤íŠ¸ ì •ì œ**: ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°

**ì‹¤ë¬´ í™œìš©:**
- ë°ì´í„° ê²€ì¦ (ì´ë©”ì¼, ì „í™”ë²ˆí˜¸)
- ë¡œê·¸ íŒŒì¼ íŒŒì‹±
- ì›¹ í¬ë¡¤ë§
- í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬

### ë°°ìš´ ì†Œí”„íŠ¸ ìŠ¤í‚¬

#### 1. ì ˆì°¨ì  â†’ ê°ì²´ì§€í–¥ ì‚¬ê³ ë°©ì‹ ì „í™˜

**ì¸ì‚¬ì´íŠ¸:**
- ì½”ë“œë¥¼ "ì‘ì—… ìˆœì„œ"ê°€ ì•„ë‹Œ "ì±…ì„ ë‹¨ìœ„"ë¡œ ë‚˜ëˆ”
- "ë¬´ì—‡ì„ í•˜ëŠ”ê°€?"ì—ì„œ "ëˆ„ê°€ ë¬´ì—‡ì„ ë‹´ë‹¹í•˜ëŠ”ê°€?"ë¡œ ì‚¬ê³  ì „í™˜
- ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸ ì„¤ê³„

**ì ìš©:**
- í´ë˜ìŠ¤ ì„¤ê³„ ì‹œ ë‹¨ì¼ ì±…ì„ ì›ì¹™(SRP) ê³ ë ¤
- ë©”ì„œë“œëŠ” í•œ ê°€ì§€ ì¼ë§Œ ìˆ˜í–‰
- ìƒíƒœ ê´€ë¦¬ë¥¼ ëª…í™•íˆ

#### 2. API ì„¤ê³„ ì² í•™

**ì¸ì‚¬ì´íŠ¸:**
- ì‚¬ìš©ì ê´€ì ì—ì„œ ìƒê° (ë¬´ì—‡ì„ ì…ë ¥í•˜ê³  ë¬´ì—‡ì„ ë°›ëŠ”ê°€?)
- ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€ ì œê³µ
- ìœ ì—°ì„± (ì»¤ìŠ¤í„°ë§ˆì´ì§• ì˜µì…˜ ì œê³µ)

**ì ìš©:**
- Query íŒŒë¼ë¯¸í„°ë¡œ ì˜µì…˜ ì œê³µ
- ê¸°ë³¸ê°’ ì„¤ì •ìœ¼ë¡œ ê°„í¸í•œ ì‚¬ìš© ë³´ì¥
- ë¬¸ì„œí™” (docstring, Swagger)

#### 3. ë””ë²„ê¹… ë° ë¬¸ì œ í•´ê²°

**ì „ëµ:**
1. ë¬¸ì œ ë¶„ë¦¬ (Gateway? mlservice? NLTK?)
2. ì§ì ‘ ì ‘ê·¼ìœ¼ë¡œ ê²€ì¦ (Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ í™•ì¸)
3. ë¡œê·¸ í™•ì¸ (`docker compose logs`)
4. ì‘ì€ ë‹¨ìœ„ë¡œ í…ŒìŠ¤íŠ¸ (ë©”ì„œë“œë³„ í…ŒìŠ¤íŠ¸)

**ì ìš©:**
- ê° ë ˆì´ì–´ë³„ë¡œ í…ŒìŠ¤íŠ¸
- ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ê¼¼ê¼¼íˆ ì½ê¸°
- ê°€ì •í•˜ì§€ ë§ê³  í™•ì¸í•˜ê¸°

### ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸

#### 1. ë°ì´í„° ì‹œê°í™”ì˜ ê°€ì¹˜

**ì¸ì‚¬ì´íŠ¸:**
- ìˆ«ì ë‚˜ì—´ë³´ë‹¤ ì´ë¯¸ì§€ê°€ 10ë°° ì´í•´í•˜ê¸° ì‰¬ì›€
- ì›Œë“œí´ë¼ìš°ë“œëŠ” ë¹„ê¸°ìˆ ìë„ ì¦‰ì‹œ ì´í•´ ê°€ëŠ¥
- ì‹œê°í™” = ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ë„êµ¬

**ì ìš©:**
- ë°ì´í„° ë¶„ì„ ê²°ê³¼ëŠ” í•­ìƒ ì‹œê°í™”
- ëŒ€ì‹œë³´ë“œì— ì›Œë“œí´ë¼ìš°ë“œ ì¶”ê°€
- ë³´ê³ ì„œì— ì§ê´€ì ì¸ ì´ë¯¸ì§€ í¬í•¨

#### 2. ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸ì˜ ì¤‘ìš”ì„±

**ì¸ì‚¬ì´íŠ¸:**
- NLPService í´ë˜ìŠ¤ëŠ” ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì—ì„œë„ ì‚¬ìš© ê°€ëŠ¥
- í•œ ë²ˆ ì˜ ë§Œë“¤ë©´ ê³„ì† í™œìš© ê°€ëŠ¥
- ê°œë°œ ì†ë„ í–¥ìƒ = ë¹„ìš© ì ˆê°

**ì ìš©:**
- ë¼ì´ë¸ŒëŸ¬ë¦¬í™” ê°€ëŠ¥í•œ ì½”ë“œ ì‘ì„±
- ì˜ì¡´ì„± ìµœì†Œí™”
- ë¬¸ì„œí™”ë¡œ ì¬ì‚¬ìš©ì„± í–¥ìƒ

#### 3. í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡° ì„¤ê³„

**ì¸ì‚¬ì´íŠ¸:**
- `app/nlp/emma/`ì™€ `app/nlp/samsung/` êµ¬ì¡°
- í–¥í›„ í•œêµ­ì–´ NLP ì¶”ê°€ ì˜ˆì •
- ëª¨ë“ˆí™”ë¡œ í™•ì¥ ìš©ì´

**ì ìš©:**
- ì²˜ìŒë¶€í„° í™•ì¥ì„± ê³ ë ¤
- í´ë” êµ¬ì¡°ë¥¼ ì˜ë¯¸ ìˆê²Œ
- ê³µí†µ ê¸°ëŠ¥ì€ ìƒìœ„ í´ë˜ìŠ¤ë¡œ

---

## ë‹¤ìŒ í•™ìŠµ ê³¼ì œ

### 1. í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬

#### 1-1. KoNLPy í†µí•©
```python
# app/nlp/samsung/korean_nlp.py
from konlpy.tag import Okt, Komoran, Mecab

class KoreanNLPService:
    def __init__(self):
        self.okt = Okt()  # ë¹ ë¥´ê³  ì •í™•
        self.komoran = Komoran()  # ì •í™•ë„ ë†’ìŒ
    
    def tokenize_korean(self, text: str):
        """í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„"""
        return self.okt.morphs(text)
    
    def extract_nouns(self, text: str):
        """ëª…ì‚¬ ì¶”ì¶œ"""
        return self.okt.nouns(text)
    
    def pos_tag(self, text: str):
        """í’ˆì‚¬ íƒœê¹…"""
        return self.okt.pos(text)
```

**í•™ìŠµ ëª©í‘œ:**
- í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ë¹„êµ
- ë„ì–´ì“°ê¸° ì—†ëŠ” í•œêµ­ì–´ ì²˜ë¦¬
- í•œêµ­ì–´ ì›Œë“œí´ë¼ìš°ë“œ (í•œê¸€ í°íŠ¸ ì ìš©)

#### 1-2. í•œêµ­ì–´ ì›Œë“œí´ë¼ìš°ë“œ
```python
def generate_korean_wordcloud(self, text: str):
    """í•œêµ­ì–´ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
    # 1. ëª…ì‚¬ ì¶”ì¶œ
    nouns = self.okt.nouns(text)
    
    # 2. ë¶ˆìš©ì–´ ì œê±°
    stopwords = ['ê²ƒ', 'ë“±', 'ë°', 'ìˆ˜', 'ë•Œë¬¸']
    filtered = [w for w in nouns if w not in stopwords and len(w) > 1]
    
    # 3. ë¹ˆë„ ë¶„ì„
    fd = FreqDist(filtered)
    
    # 4. ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± (í•œê¸€ í°íŠ¸ ì§€ì •)
    wc = WordCloud(
        font_path='app/nlp/data/D2Coding.ttf',
        width=1000,
        height=600,
        background_color='white'
    )
    wc.generate_from_frequencies(fd)
    
    return wc
```

#### 1-3. í•œêµ­ì–´ ë§ë­‰ì¹˜ ë¶„ì„
```python
# data/kr-Report_2018.txt ë¶„ì„
@router.get("/korean/wordcloud")
async def generate_korean_wordcloud():
    """í•œêµ­ì–´ ë¦¬í¬íŠ¸ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
    with open('app/nlp/data/kr-Report_2018.txt', 'r', encoding='utf-8') as f:
        text = f.read()
    
    korean_nlp = KoreanNLPService()
    wc = korean_nlp.generate_korean_wordcloud(text)
    
    # ... (ì´ë¯¸ì§€ ë³€í™˜ ë° ë°˜í™˜)
```

### 2. ê°ì • ë¶„ì„ (Sentiment Analysis)

#### 2-1. VADER ê°ì • ë¶„ì„
```python
from nltk.sentiment import SentimentIntensityAnalyzer

class SentimentService:
    def __init__(self):
        nltk.download('vader_lexicon')
        self.sia = SentimentIntensityAnalyzer()
    
    def analyze_sentiment(self, text: str) -> Dict[str, float]:
        """
        ê°ì • ë¶„ì„
        
        Returns:
            {
                'neg': 0.1,   # ë¶€ì •
                'neu': 0.6,   # ì¤‘ë¦½
                'pos': 0.3,   # ê¸ì •
                'compound': 0.4  # ì¢…í•© ì ìˆ˜ (-1 ~ 1)
            }
        """
        return self.sia.polarity_scores(text)
```

#### 2-2. ë¦¬ë·° ê°ì • ë¶„ì„ API
```python
@router.post("/sentiment")
async def analyze_text_sentiment(text: str = Query(...)):
    """í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„"""
    sentiment_service = SentimentService()
    scores = sentiment_service.analyze_sentiment(text)
    
    # ë¶„ë¥˜
    if scores['compound'] >= 0.05:
        label = "ê¸ì •"
    elif scores['compound'] <= -0.05:
        label = "ë¶€ì •"
    else:
        label = "ì¤‘ë¦½"
    
    return {
        "text": text,
        "scores": scores,
        "label": label
    }
```

#### 2-3. ëŒ€ëŸ‰ ë¦¬ë·° ë¶„ì„
```python
@router.post("/sentiment/batch")
async def analyze_batch_sentiment(reviews: List[str]):
    """ì—¬ëŸ¬ ë¦¬ë·° ì¼ê´„ ë¶„ì„"""
    results = []
    for review in reviews:
        scores = sentiment_service.analyze_sentiment(review)
        results.append({
            "review": review,
            "compound": scores['compound'],
            "label": "ê¸ì •" if scores['compound'] >= 0.05 else "ë¶€ì •"
        })
    
    # í†µê³„
    positive_count = sum(1 for r in results if r['label'] == "ê¸ì •")
    negative_count = sum(1 for r in results if r['label'] == "ë¶€ì •")
    
    return {
        "results": results,
        "statistics": {
            "total": len(reviews),
            "positive": positive_count,
            "negative": negative_count,
            "positive_rate": positive_count / len(reviews)
        }
    }
```

### 3. Named Entity Recognition (NER)

#### 3-1. ê°œì²´ëª… ì¸ì‹
```python
import spacy

class NERService:
    def __init__(self):
        # spaCy ëª¨ë¸ ë¡œë“œ
        self.nlp = spacy.load('en_core_web_sm')
    
    def extract_entities(self, text: str):
        """ê°œì²´ëª… ì¶”ì¶œ"""
        doc = self.nlp(text)
        
        entities = []
        for ent in doc.ents:
            entities.append({
                "text": ent.text,
                "label": ent.label_,  # PERSON, ORG, GPE, DATE ë“±
                "start": ent.start_char,
                "end": ent.end_char
            })
        
        return entities
```

#### 3-2. ì—”í‹°í‹° ì‹œê°í™”
```python
from spacy import displacy

@router.get("/ner")
async def extract_named_entities(text: str = Query(...)):
    """ê°œì²´ëª… ì¸ì‹ ë° ì‹œê°í™”"""
    ner_service = NERService()
    doc = ner_service.nlp(text)
    
    # HTML ì‹œê°í™” ìƒì„±
    html = displacy.render(doc, style="ent", page=False)
    
    return Response(content=html, media_type="text/html")
```

### 4. í…ìŠ¤íŠ¸ ìš”ì•½ (Text Summarization)

#### 4-1. ì¶”ì¶œì  ìš”ì•½
```python
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize
from nltk.probability import FreqDist

class SummarizationService:
    def extractive_summary(self, text: str, num_sentences: int = 3):
        """ì¶”ì¶œì  ìš”ì•½ (ì¤‘ìš” ë¬¸ì¥ ì¶”ì¶œ)"""
        # 1. ë¬¸ì¥ í† í°í™”
        sentences = sent_tokenize(text)
        
        # 2. ë‹¨ì–´ í† í°í™” ë° ë¶ˆìš©ì–´ ì œê±°
        words = word_tokenize(text.lower())
        stop_words = set(stopwords.words('english'))
        words = [w for w in words if w.isalnum() and w not in stop_words]
        
        # 3. ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        freq_dist = FreqDist(words)
        
        # 4. ë¬¸ì¥ ì ìˆ˜ ê³„ì‚° (ë¬¸ì¥ì— í¬í•¨ëœ ë‹¨ì–´ì˜ ë¹ˆë„ í•©)
        sentence_scores = {}
        for sentence in sentences:
            for word in word_tokenize(sentence.lower()):
                if word in freq_dist:
                    sentence_scores[sentence] = sentence_scores.get(sentence, 0) + freq_dist[word]
        
        # 5. ìƒìœ„ Nê°œ ë¬¸ì¥ ì„ íƒ
        summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]
        
        # 6. ì›ë³¸ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        summary = [s for s in sentences if s in summary_sentences]
        
        return ' '.join(summary)
```

#### 4-2. ìš”ì•½ API
```python
@router.post("/summarize")
async def summarize_text(
    text: str = Query(...),
    num_sentences: int = Query(3, description="ìš”ì•½ ë¬¸ì¥ ìˆ˜")
):
    """í…ìŠ¤íŠ¸ ìš”ì•½"""
    summarization_service = SummarizationService()
    summary = summarization_service.extractive_summary(text, num_sentences)
    
    return {
        "original_length": len(text),
        "summary_length": len(summary),
        "summary": summary,
        "compression_ratio": len(summary) / len(text)
    }
```

### 5. ì„±ëŠ¥ ìµœì í™”

#### 5-1. ìºì‹± ì „ëµ
```python
from functools import lru_cache
import redis

class CachedNLPService(NLPService):
    def __init__(self):
        super().__init__()
        self.redis_client = redis.Redis(host='redis', port=6379)
    
    @lru_cache(maxsize=100)
    def tokenize_regex(self, text: str):
        """ë©”ëª¨ë¦¬ ìºì‹±"""
        return super().tokenize_regex(text)
    
    def generate_wordcloud_cached(self, corpus_id: str, **kwargs):
        """Redis ìºì‹±"""
        cache_key = f"wordcloud:{corpus_id}:{hash(frozenset(kwargs.items()))}"
        
        # ìºì‹œ í™•ì¸
        cached = self.redis_client.get(cache_key)
        if cached:
            return cached
        
        # ìƒì„± ë° ìºì‹±
        wc = self.generate_wordcloud(**kwargs)
        img_bytes = wc.to_image().tobytes()
        self.redis_client.setex(cache_key, 3600, img_bytes)  # 1ì‹œê°„ TTL
        
        return img_bytes
```

#### 5-2. ë¹„ë™ê¸° ì²˜ë¦¬
```python
import asyncio
from concurrent.futures import ProcessPoolExecutor

class AsyncNLPService:
    def __init__(self):
        self.executor = ProcessPoolExecutor(max_workers=4)
    
    async def process_multiple_texts(self, texts: List[str]):
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ë³‘ë ¬ ì²˜ë¦¬"""
        loop = asyncio.get_event_loop()
        tasks = [
            loop.run_in_executor(self.executor, self.process_text, text)
            for text in texts
        ]
        results = await asyncio.gather(*tasks)
        return results
```

### 6. í”„ë¡ íŠ¸ì—”ë“œ í†µí•©

#### 6-1. React ì›Œë“œí´ë¼ìš°ë“œ ë·°ì–´
```jsx
import React, { useState, useEffect } from 'react';

function WordCloudViewer() {
  const [imageUrl, setImageUrl] = useState('');
  const [loading, setLoading] = useState(false);
  
  const generateWordCloud = async () => {
    setLoading(true);
    const response = await fetch(
      'http://localhost:8080/api/ml/nlp/emma?width=1920&height=1080'
    );
    const blob = await response.blob();
    const url = URL.createObjectURL(blob);
    setImageUrl(url);
    setLoading(false);
  };
  
  return (
    <div>
      <button onClick={generateWordCloud}>
        Generate WordCloud
      </button>
      {loading && <p>Loading...</p>}
      {imageUrl && <img src={imageUrl} alt="WordCloud" />}
    </div>
  );
}
```

#### 6-2. ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ë¶„ì„ ëŒ€ì‹œë³´ë“œ
```jsx
function TextAnalysisDashboard() {
  const [text, setText] = useState('');
  const [stats, setStats] = useState(null);
  
  useEffect(() => {
    // ë””ë°”ìš´ìŠ¤ í›„ API í˜¸ì¶œ
    const timer = setTimeout(async () => {
      if (text.length > 10) {
        const response = await fetch(
          `http://localhost:8080/api/ml/nlp/emma/stats`
        );
        const data = await response.json();
        setStats(data);
      }
    }, 500);
    
    return () => clearTimeout(timer);
  }, [text]);
  
  return (
    <div>
      <textarea 
        value={text} 
        onChange={(e) => setText(e.target.value)}
      />
      {stats && (
        <div>
          <p>Total Words: {stats.total_words}</p>
          <p>Top Word: {stats.top_10_words[0].word}</p>
        </div>
      )}
    </div>
  );
}
```

---

## ì°¸ê³  ìë£Œ ë° ë” ë°°ìš°ê¸°

### ê³µì‹ ë¬¸ì„œ
- [NLTK ê³µì‹ ë¬¸ì„œ](https://www.nltk.org/)
- [NLTK Book (ë¬´ë£Œ ì˜¨ë¼ì¸)](https://www.nltk.org/book/)
- [WordCloud ë¬¸ì„œ](https://amueller.github.io/word_cloud/)
- [KoNLPy ë¬¸ì„œ](https://konlpy.org/)
- [spaCy ë¬¸ì„œ](https://spacy.io/)

### ì¶”ì²œ í•™ìŠµ ìë£Œ
- **ì±…**: "Natural Language Processing with Python" (NLTK Book)
- **ì±…**: "Speech and Language Processing" by Jurafsky & Martin
- **ì˜¨ë¼ì¸ ê°•ì˜**: Coursera "Natural Language Processing"
- **ìœ íŠœë¸Œ**: "StatQuest with Josh Starmer" (NLP ê°œë… ì„¤ëª…)

### ìœ ìš©í•œ ë„êµ¬
- **NLTK Downloader GUI**: ë°ì´í„° ê´€ë¦¬ ë„êµ¬
- **spaCy CLI**: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ê´€ë¦¬
- **Jupyter Notebook**: ëŒ€í™”í˜• NLP ì‹¤í—˜
- **Postman**: API í…ŒìŠ¤íŠ¸

---

## ë§ˆë¬´ë¦¬

ì˜¤ëŠ˜ ìš°ë¦¬ëŠ” ì ˆì°¨ì  NLTK ì½”ë“œë¥¼ ê°ì²´ì§€í–¥ ì„¤ê³„ì˜ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ NLP ì„œë¹„ìŠ¤ë¡œ ë°œì „ì‹œì¼°ìŠµë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ:

1. **ë¦¬íŒ©í† ë§**: ì ˆì°¨ì  ì½”ë“œ â†’ OOP í´ë˜ìŠ¤ êµ¬ì¡°
2. **ìì—°ì–´ ì²˜ë¦¬**: NLTKì˜ í† í°í™”, í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ë¹ˆë„ ë¶„ì„
3. **ì‹œê°í™”**: ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ë° ìë™ ì €ì¥
4. **API ì„¤ê³„**: FastAPI ì´ë¯¸ì§€ ì‘ë‹µ, Query íŒŒë¼ë¯¸í„°
5. **ë°°í¬**: Docker í™˜ê²½ì—ì„œ NLTK ë°ì´í„° ê´€ë¦¬

ì´ì œ Emma ì†Œì„¤ì˜ ë“±ì¥ì¸ë¬¼ ê´€ê³„ë¥¼ ì›Œë“œí´ë¼ìš°ë“œë¡œ í•œëˆˆì— íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë” ë‚˜ì•„ê°€ í•œêµ­ì–´ NLP, ê°ì • ë¶„ì„, ê°œì²´ëª… ì¸ì‹ ë“±ìœ¼ë¡œ í™•ì¥í•  ìˆ˜ ìˆëŠ” ê²¬ê³ í•œ ê¸°ë°˜ì„ ë§ˆë ¨í–ˆìŠµë‹ˆë‹¤.

NLPëŠ” ë‹¨ìˆœíˆ "ë‹¨ì–´ë¥¼ ì„¸ëŠ” ê²ƒ"ì´ ì•„ë‹ˆë¼ "ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒ"ì…ë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ìì—°ì–´ ì²˜ë¦¬ì˜ ê¸°ì´ˆë¥¼ ë‹¤ì¡Œìœ¼ë‹ˆ, ì´ì œ ì‹¤ì „ í”„ë¡œì íŠ¸ì— ì ìš©í•´ë³´ì„¸ìš”!

**Happy NLP Coding! ğŸ”¤ğŸ“Š**

